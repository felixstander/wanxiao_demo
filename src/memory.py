import hashlib
import os
import sqlite3
import struct
from pathlib import Path
from typing import Dict, List

import sqlite_vec
from dotenv import load_dotenv
from openai import OpenAI

# 1. è·å–å½“å‰è„šæœ¬æ‰€åœ¨çš„ç›®å½• (src)
current_dir = Path(__file__).resolve().parent
# 2. è·å–ä¸Šä¸€çº§ç›®å½• (é¡¹ç›®æ ¹ç›®å½•)
parent_dir = current_dir.parent
# 3. å®šä¹‰ç›®æ ‡æ–‡ä»¶å¤¹è·¯å¾„
target_memory_folder = parent_dir / ".memories"
target_memory_folder.mkdir(parents=True, exist_ok=True)

load_dotenv()

# === é…ç½® ===
DB_PATH = target_memory_folder / "wanxiao_memory_chunked.db"
API_KEY = os.getenv("OPENROUTER_API_KEY")
EMBEDDING_MODEL = "qwen/qwen3-embedding-8b"  # å‡è®¾ OpenRouter æ”¯æŒæ­¤ Embedding æ¨¡å‹
EMBEDDING_DIM = 4096  # âš ï¸ éœ€æ ¹æ®å®é™…æ¨¡å‹ç¡®è®¤

# === åˆ†å—é…ç½® (æ¨¡æ‹Ÿ 400 token / 80 overlap) ===
# å‡è®¾å¹³å‡ä¸€è¡Œ 10-15 tokenï¼Œæˆ‘ä»¬è®¾å®šï¼š
CHUNK_SIZE_LINES = 15  # çº¦ 300-450 token
OVERLAP_LINES = 3  # çº¦ 50-80 token

client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=API_KEY,
    default_headers={"HTTP-Referer": "https://localhost", "X-Title": "LocalAgent"},
)


class MemoryStore:
    def __init__(self):
        self.conn = sqlite3.connect(DB_PATH)
        self.conn.enable_load_extension(True)
        sqlite_vec.load(self.conn)
        self.conn.enable_load_extension(False)
        self._init_schema()

    def _init_schema(self):
        cursor = self.conn.cursor()

        # 1. åŸºç¡€ä¿¡æ¯è¡¨ (å¢åŠ äº† start_line, end_line)
        cursor.execute(
            """
            CREATE TABLE IF NOT EXISTS chunks (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                file_path TEXT NOT NULL,
                start_line INTEGER NOT NULL,
                end_line INTEGER NOT NULL,
                content TEXT NOT NULL,
                chunk_hash TEXT NOT NULL,
                created_at INTEGER DEFAULT (unixepoch())
            );
        """
        )
        # ä¸º file_path å»ºç´¢å¼•ï¼Œæ–¹ä¾¿åˆ é™¤æ—§æ•°æ®
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_filepath ON chunks(file_path);")

        # 2. å‘é‡è¡¨
        cursor.execute(
            f"""
            CREATE VIRTUAL TABLE IF NOT EXISTS chunks_vec USING vec0(
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                embedding float[{EMBEDDING_DIM}]
            );
        """
        )

        # 3. å…¨æ–‡æœç´¢è¡¨(FTS5)
        cursor.execute(
            """
            CREATE VIRTUAL TABLE IF NOT EXISTS chunks_fts USING fts5(
                content,
                tokenize='porter'
            );
        """
        )

        # 4. ç¼“å­˜è¡¨
        cursor.execute(
            f"""
            CREATE TABLE IF NOT EXISTS embedding_cache (
                hash TEXT PRIMARY KEY,
                embedding BLOB
            );
        """
        )
        self.conn.commit()

    def _calculate_hash(self, text: str) -> str:
        return hashlib.md5(text.encode("utf-8")).hexdigest()

    def _get_embedding(self, text: str, text_hash: str) -> List[float]:
        """è·å–å‘é‡ (Cache -> OpenRouter API)"""
        cursor = self.conn.cursor()

        # A. æŸ¥ç¼“å­˜
        cursor.execute(
            "SELECT embedding FROM embedding_cache WHERE hash = ?", (text_hash,)
        )
        row = cursor.fetchone()
        if row:
            return list(struct.unpack(f"{EMBEDDING_DIM}f", row[0]))

        # B. è°ƒ API
        try:
            print(f"ğŸ“¡ Calling OpenRouter for embedding...")
            resp = client.embeddings.create(
                model=EMBEDDING_MODEL,
                input=text,
                encoding_format="float",
            )
            vector = resp.data[0].embedding

            # ç»´åº¦æ£€æŸ¥ (é˜²æ­¢æ¨¡å‹è¿”å› 1024 ç»´ä½†æˆ‘ä»¬è¡¨æ˜¯ 1536 ç»´)
            if len(vector) != EMBEDDING_DIM:
                raise ValueError(
                    f"ç»´åº¦ä¸åŒ¹é…! æ¨¡å‹è¿”å› {len(vector)}, æ•°æ®åº“å®šä¹‰ {EMBEDDING_DIM}"
                )

            # å­˜ç¼“å­˜
            vec_blob = struct.pack(f"{EMBEDDING_DIM}f", *vector)
            cursor.execute(
                "INSERT OR REPLACE INTO embedding_cache (hash, embedding) VALUES (?, ?)",
                (text_hash, vec_blob),
            )
            self.conn.commit()

            return vector
        except Exception as e:
            print(f"âŒ OpenRouter API Error: {e}")
            # è¿”å›é›¶å‘é‡é˜²æ­¢ç¨‹åºå´©æºƒï¼Œä½†åœ¨ç”Ÿäº§ä¸­åº”è¯¥æŠ›å‡ºå¼‚å¸¸æˆ–é‡è¯•
            return [0.0] * EMBEDDING_DIM

    def _split_text_sliding_window(self, text: str) -> List[Dict]:
        """
        è¯­ä¹‰åˆ†å—ï¼šæ ¹æ® '## ' æ ‡è®°è¿›è¡Œåˆ‡åˆ†ã€‚
        ç‰¹æ€§ï¼šä¼šè‡ªåŠ¨å°†æ–‡ä»¶å¤´éƒ¨çš„æ—¥æœŸï¼ˆç¬¬ä¸€ä¸ª ## ä¹‹å‰çš„å†…å®¹ï¼‰æ‹¼æ¥åˆ°æ¯ä¸ªå—ä¸­ï¼Œ
        ç¡®ä¿æ¯ä¸ªå—éƒ½æœ‰æ—¥æœŸä¸Šä¸‹æ–‡ã€‚
        """
        lines = text.split("\n")
        total_lines = len(lines)
        chunks = []

        if total_lines == 0:
            return []

        # === æ­¥éª¤ 1: æå–å…¨å±€ä¸Šä¸‹æ–‡ (Global Context) ===
        # é€šå¸¸æ˜¯æ–‡ä»¶çš„ç¬¬ä¸€è¡Œï¼Œä¾‹å¦‚ "# 2026-02-10"
        # æˆ‘ä»¬æŠŠç¬¬ä¸€ä¸ª "## " å‡ºç°ä¹‹å‰çš„æ‰€æœ‰å†…å®¹éƒ½è§†ä¸º Context
        header_context = ""
        body_start_index = 0

        for i, line in enumerate(lines):
            if line.strip().startswith("## "):
                header_context = "\n".join(lines[:i]).strip()
                body_start_index = i
                break
        else:
            # å¦‚æœæ•´ä¸ªæ–‡ä»¶æ²¡æœ‰ "## "ï¼Œåˆ™æŠŠå…¨æ–‡å½“ä½œä¸€ä¸ªå—
            return [{"content": text, "start": 1, "end": total_lines}]

        # === æ­¥éª¤ 2: åŸºäº ## éå†åˆ‡åˆ† ===
        current_chunk_lines = []
        # è®°å½•å½“å‰å—åœ¨åŸå§‹æ–‡ä»¶ä¸­çš„èµ·å§‹è¡Œå· (1-based)
        current_chunk_start = body_start_index + 1

        for i in range(body_start_index, total_lines):
            line = lines[i]
            is_header = line.strip().startswith("## ")

            # å¦‚æœé‡åˆ°äº†æ–°çš„ Headerï¼Œä¸”å½“å‰ç¼“å­˜é‡Œå·²æœ‰å†…å®¹ï¼Œè¯´æ˜ä¸Šä¸€å—ç»“æŸäº†
            if is_header and current_chunk_lines:
                # A. ç»„è£…ä¸Šä¸€å—çš„å†…å®¹
                # æ ¼å¼: [æ—¥æœŸå¤´] + [æ¢è¡Œ] + [äº‹ä»¶å†…å®¹]
                chunk_text = (
                    header_context + "\n\n" + "\n".join(current_chunk_lines)
                ).strip()

                chunks.append(
                    {
                        "content": chunk_text,
                        "start": current_chunk_start,
                        "end": i,  # ä¸Šä¸€å—ç»“æŸäºå½“å‰è¡Œä¹‹å‰ (i æ˜¯ 0-basedï¼Œä½†åœ¨è¡Œå·é€»è¾‘é‡Œæ­£å¥½ä»£è¡¨ä¸Šä¸€è¡Œçš„ç»“æŸ)
                    }
                )

                # B. é‡ç½®ï¼Œå¼€å§‹æ–°çš„ä¸€å—
                current_chunk_lines = [line]  # æŠŠå½“å‰çš„ Header (## 10:00 AM...) æ”¾è¿›å»
                current_chunk_start = i + 1  # è®°å½•æ–°å—çš„èµ·å§‹è¡Œå·
            else:
                # å¦åˆ™åªæ˜¯æ™®é€šå†…å®¹ï¼ŒåŠ å…¥å½“å‰å—
                current_chunk_lines.append(line)

        # === æ­¥éª¤ 3: å¤„ç†æœ€åä¸€å— ===
        if current_chunk_lines:
            chunk_text = (
                header_context + "\n\n" + "\n".join(current_chunk_lines)
            ).strip()
            chunks.append(
                {
                    "content": chunk_text,
                    "start": current_chunk_start,
                    "end": total_lines,
                }
            )

        return chunks

    def process_file(self, file_path: str, full_content: str):
        """
        å¤„ç†æ•´ä¸ªæ–‡ä»¶ï¼šæ¸…ç†æ—§è®°å½• -> åˆ‡åˆ† -> å‘é‡åŒ– -> å­˜å‚¨
        """
        print(f"Processing file: {file_path} ...")
        cursor = self.conn.cursor()

        # 1. äº‹åŠ¡å¼€å§‹ï¼šå…ˆåˆ é™¤è¯¥æ–‡ä»¶ä¹‹å‰çš„æ‰€æœ‰è®°å½• (é˜²æ­¢æ–‡ä»¶å˜çŸ­åæ®‹ç•™æ—§å—)
        # æ³¨æ„ï¼šsqlite-vec å’Œ fts5 éœ€è¦æ ¹æ® rowid åˆ é™¤ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œ
        # å®é™…ç”Ÿäº§ä¸­å¯èƒ½éœ€è¦å…ˆæŸ¥å‡ºæ—§çš„ id åˆ—è¡¨å†åˆ é™¤ vec/fts è¡¨å¯¹åº”è¡Œã€‚
        # ä½†ä¸ºäº†æ¼”ç¤ºç®€å•ï¼Œæˆ‘ä»¬å‡è®¾ id æ˜¯è‡ªå¢ä¸”ä¸å¤ç”¨çš„ï¼Œæˆ–è€…æˆ‘ä»¬æ¥å—ä¸€å®šçš„å­¤å„¿æ•°æ®ï¼ˆå®šæœŸæ¸…ç†ï¼‰ã€‚

        # æ›´ä¸¥è°¨çš„åšæ³•æ˜¯ï¼š
        cursor.execute("SELECT id FROM chunks WHERE file_path = ?", (file_path,))
        old_ids = [row[0] for row in cursor.fetchall()]

        if old_ids:
            # æ‰¹é‡åˆ é™¤ Vector è¡¨
            for oid in old_ids:
                cursor.execute("DELETE FROM chunks_vec WHERE id = ?", (oid,))
                # FTS åˆ é™¤ç¨å¾®éº»çƒ¦ç‚¹ï¼Œé€šå¸¸ FTS ä¸éœ€è¦æ˜¾å¼åˆ ï¼Œæˆ–è€…é€šè¿‡ trigger ç»´æŠ¤
                # è¿™é‡Œæˆ‘ä»¬ç®€å•åœ°åªåœ¨ä¸»è¡¨ä¸­ç»´æŠ¤å…³ç³»

            # åˆ é™¤ä¸»è¡¨
            cursor.execute("DELETE FROM chunks WHERE file_path = ?", (file_path,))

        # 2. åˆ‡åˆ†æ–‡ä»¶
        chunks = self._split_text_sliding_window(full_content)
        print(f"  - Split into {len(chunks)} chunks.")

        # 3. é€ä¸ªå†™å…¥
        for chunk in chunks:
            content = chunk["content"]
            if not content.strip():
                continue  # è·³è¿‡ç©ºå—

            chunk_hash = self._calculate_hash(content)
            vector = self._get_embedding(content, chunk_hash)

            # A. å†™å…¥ä¸»è¡¨ chunks
            cursor.execute(
                """
                INSERT INTO chunks (file_path, start_line, end_line, content, chunk_hash)
                VALUES (?, ?, ?, ?, ?)
            """,
                (file_path, chunk["start"], chunk["end"], content, chunk_hash),
            )
            row_id = cursor.lastrowid

            # B. å†™å…¥ FTS
            cursor.execute(
                "INSERT INTO chunks_fts (rowid, content) VALUES (?, ?)",
                (row_id, content),
            )

            # C. å†™å…¥ Vector
            vec_blob = struct.pack(f"{EMBEDDING_DIM}f", *vector)
            cursor.execute(
                "INSERT INTO chunks_vec (id, embedding) VALUES (?, ?)",
                (row_id, vec_blob),
            )

        self.conn.commit()
        print(f"âœ… {file_path} indexed successfully.")

    def search(self, query: str, limit: int = 5, alpha: float = 0.7):
        """æ··åˆæ£€ç´¢é€»è¾‘ (ä¿æŒä¸å˜)"""
        query_hash = self._calculate_hash(query)
        query_vec = self._get_embedding(query, query_hash)

        if all(v == 0.0 for v in query_vec):
            return []

        cursor = self.conn.cursor()
        query_blob = struct.pack(f"{EMBEDDING_DIM}f", *query_vec)

        # 1. å‘é‡æœç´¢
        cursor.execute(
            """
            SELECT id, distance FROM chunks_vec 
            WHERE embedding MATCH ? AND k = ? ORDER BY distance
        """,
            (query_blob, limit * 2),
        )
        vec_res = {r[0]: r[1] for r in cursor.fetchall()}

        # 2. å…³é”®è¯æœç´¢
        cursor.execute(
            """
            SELECT rowid, rank FROM chunks_fts 
            WHERE chunks_fts MATCH ? ORDER BY rank LIMIT ?
        """,
            (query, limit * 2),
        )
        fts_res = {r[0]: r[1] for r in cursor.fetchall()}

        # 3. èåˆ
        all_ids = set(vec_res.keys()) | set(fts_res.keys())
        scores = []

        # æç®€å½’ä¸€åŒ–å‡½æ•°
        def norm(val, min_v, max_v):
            if max_v == min_v:
                return 0.0
            return (val - min_v) / (max_v - min_v)

        v_vals = vec_res.values() or [0]
        f_vals = fts_res.values() or [0]
        v_min, v_max = min(v_vals), max(v_vals)
        f_min, f_max = min(f_vals), max(f_vals)

        for rid in all_ids:
            # ç¼ºå¤±å€¼å¤„ç†: æ²¡å‘½ä¸­çš„ç»™æœ€å·®åˆ†
            v_raw = vec_res.get(rid, v_max)
            f_raw = fts_res.get(rid, f_max)

            # å½’ä¸€åŒ– (è¶Šå°è¶Šå¥½ -> è½¬ä¸ºè¶Šå¤§è¶Šå¥½: 1 - norm)
            v_score = 1.0 - norm(v_raw, v_min, v_max)
            f_score = 1.0 - norm(f_raw, f_min, f_max)

            # text{æœ€ç»ˆå¾—åˆ†} = (0.7 \times \text{å‘é‡è¯­ä¹‰å¾—åˆ†}) + (0.3 \times \text{BM25 å…³é”®è¯å¾—åˆ†})
            final = (alpha * v_score) + ((1 - alpha) * f_score)
            scores.append((rid, final))

        scores.sort(key=lambda x: x[1], reverse=True)
        # === ç»“æœå±•ç¤º ===
        results = []
        for rid, score in scores[:limit]:
            cursor.execute(
                "SELECT file_path, start_line, end_line, content FROM chunks WHERE id = ?",
                (rid,),
            )
            row = cursor.fetchone()
            if row:
                results.append(
                    {
                        "score": round(score, 4),
                        "source": f"{row[0]} (L{row[1]}-{row[2]})",  # å…³é”®ï¼šæ˜¾ç¤ºè¡Œå·
                        "content": row[3][:100] + "...",  # é¢„è§ˆ
                    }
                )

        return results


# === æµ‹è¯•è¿è¡Œ ===
if __name__ == "__main__":
    store = MemoryStore()

    # memory_path = parent_dir / "memory/2026-02-10.md"
    # with open(parent_dir / memory_path, "r", encoding="utf-8") as f:
    #     memory = f.read()
    #
    # # å†™å…¥æ–‡ä»¶ï¼ˆæ¨¡æ‹Ÿï¼‰
    # store.process_file(str(memory_path), memory)

    text = "åˆšåˆšèŠäº†ä»€ä¹ˆé«˜æ½œå®¢æˆ·æ¥ç€ï¼Ÿ"
    print(f"\n--- {text} ---")
    results = store.search(text, limit=2)
    for r in results:
        print(f"[{r['score']}] {r['source']}\n    {r['content']}")
