import csv
import json
import os
import sys
import threading
import time
import uuid
from datetime import date, datetime, timedelta
from pathlib import Path
from typing import Any, AsyncIterator

# Patch: ä¿®æ”¹ read_file é»˜è®¤è¯»å–è¡Œæ•°é™åˆ¶ä¸º 500 è¡Œ
import deepagents.middleware.filesystem as _fs_module
import uvicorn

# å°è¯•å¯¼å…¥ Daytonaï¼ˆå¦‚æœæœªå®‰è£…åˆ™ç»™å‡ºå‹å¥½æç¤ºï¼‰
from daytona import CreateSandboxBaseParams, Daytona, FileUpload
from deepagents import create_deep_agent

_fs_module.DEFAULT_READ_LIMIT = 500
from deepagents.backends import FilesystemBackend
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException
from fastapi.responses import HTMLResponse, StreamingResponse
from fastapi.staticfiles import StaticFiles
from langchain_daytona import DaytonaSandbox
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.memory import MemorySaver
from langgraph.store.memory import InMemoryStore
from pydantic import BaseModel, Field
from watchdog.events import FileSystemEventHandler
from watchdog.observers import Observer

from src.output_sanitize_config import load_output_sanitize_config
from src.prompt_config import PromptConfig

DAYTONA_AVAILABLE = True

# é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).resolve().parent
FRONTEND_DIR = PROJECT_ROOT / "frontend"
SKILLS_DIR = PROJECT_ROOT / "skills"
DATA_DIR = PROJECT_ROOT / "data"
MEMORIES_DIR = PROJECT_ROOT / "memories"
DAILY_DIR = MEMORIES_DIR / "daily"
LONG_TERM_FILE = MEMORIES_DIR / "MEMORY.md"

# Daytona å…¨å±€å®ä¾‹
DAYTONA_INSTANCE: Daytona | None = None
DAYTONA_SANDBOX: Any | None = None

# æ–‡ä»¶ç›‘å¬å™¨å…¨å±€å®ä¾‹
FILE_OBSERVER: Observer | None = None
SYNC_DEBOUNCE_TIMER: threading.Timer | None = None
SYNC_LOCK = threading.Lock()

load_dotenv()  # src modules already imported at lines 34-35


PROMPTS = PromptConfig(PROJECT_ROOT)
OUTPUT_SANITIZE_CONFIG = load_output_sanitize_config(
    PROJECT_ROOT / "config" / "output_sanitize.yaml"
)

model_name = PROMPTS.get_model_name("main")
memory_model_name = PROMPTS.get_model_name("memory_agent")
MEMORY_WRITE_LOCK = threading.Lock()


def _env_flag(name: str, default: bool = True) -> bool:
    raw = os.getenv(name)
    if raw is None:
        return default
    return raw.strip().lower() in {"1", "true", "yes", "on"}


def upload_directory_to_sandbox(
    sandbox, local_dir: Path, remote_base: str, label: str = "æ–‡ä»¶"
):
    """å°†æœ¬åœ°ç›®å½•ä¸Šä¼ åˆ°æ²™ç®±ã€‚"""
    print(f"\nğŸ“¤ æ­£åœ¨ä¸Šä¼  {label} æ–‡ä»¶å¤¹åˆ°æ²™ç®± {remote_base}...")

    upload_files = []

    if not local_dir.exists():
        print(f"âš ï¸  æœ¬åœ° {label} ç›®å½•ä¸å­˜åœ¨: {local_dir}")
        return

    # éå†ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶
    for file_path in local_dir.rglob("*"):
        if file_path.is_file():
            # è·³è¿‡éšè—æ–‡ä»¶å’Œç³»ç»Ÿæ–‡ä»¶
            if file_path.name.startswith(".") or file_path.name.endswith("~"):
                continue
            # è·³è¿‡ç‰¹å®šç›®å½•
            skip_dirs = {"__pycache__", ".git", ".venv", "venv", "node_modules", ".pytest_cache"}
            if any(part in skip_dirs for part in file_path.parts):
                continue

            # è®¡ç®—ç›¸å¯¹è·¯å¾„
            rel_path = file_path.relative_to(local_dir)
            remote_path = f"{remote_base}/{rel_path}"

            # è¯»å–æ–‡ä»¶å†…å®¹
            try:
                with open(file_path, "rb") as f:
                    content = f.read()

                upload_files.append(FileUpload(source=content, destination=remote_path))
            except Exception as e:
                print(f"âš ï¸  è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")

    if upload_files:
        # æ‰¹é‡ä¸Šä¼ æ–‡ä»¶
        try:
            sandbox.fs.upload_files(upload_files)
            print(f"âœ… å·²ä¸Šä¼  {len(upload_files)} ä¸ª {label} æ–‡ä»¶åˆ°æ²™ç®±")
        except Exception as e:
            print(f"âš ï¸  æ‰¹é‡ä¸Šä¼ å¤±è´¥: {e}")
            # å°è¯•é€ä¸ªä¸Šä¼ ï¼Œè·³è¿‡æœ‰é—®é¢˜çš„æ–‡ä»¶
            success_count = 0
            for upload_file in upload_files:
                try:
                    sandbox.fs.upload_files([upload_file])
                    success_count += 1
                except Exception as e2:
                    print(f"âš ï¸  è·³è¿‡æ–‡ä»¶ {upload_file.destination}: {e2}")
            print(f"âœ… æˆåŠŸä¸Šä¼  {success_count}/{len(upload_files)} ä¸ª {label} æ–‡ä»¶")
    else:
        print(f"âš ï¸  æ²¡æœ‰ {label} æ–‡ä»¶éœ€è¦ä¸Šä¼ ")


def upload_skills_to_sandbox(
    sandbox, local_skills_dir: Path, remote_base: str = "/home/daytona/skills"
):
    """å°† skills æ–‡ä»¶å¤¹ä¸Šä¼ åˆ°æ²™ç®±ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰ã€‚"""
    upload_directory_to_sandbox(sandbox, local_skills_dir, remote_base, "skills")


_SANDBOX_ID_FILE = Path(__file__).resolve().parent / ".daytona_sandbox_id"


def _save_sandbox_id(sandbox_id: str):
    """ä¿å­˜æ²™ç®± ID åˆ°æ–‡ä»¶ï¼Œä¾›å…¶ä»–è¿›ç¨‹è¯»å–ã€‚"""
    try:
        _SANDBOX_ID_FILE.write_text(sandbox_id, encoding="utf-8")
    except Exception as e:
        print(f"âš ï¸  ä¿å­˜æ²™ç®± ID å¤±è´¥: {e}")


def _load_sandbox_id() -> str | None:
    """ä»æ–‡ä»¶è¯»å–æ²™ç®± IDã€‚"""
    try:
        if _SANDBOX_ID_FILE.exists():
            return _SANDBOX_ID_FILE.read_text(encoding="utf-8").strip()
    except Exception:
        pass
    return None


def _clear_sandbox_id():
    """æ¸…é™¤æ²™ç®± ID æ–‡ä»¶ã€‚"""
    try:
        if _SANDBOX_ID_FILE.exists():
            _SANDBOX_ID_FILE.unlink()
    except Exception:
        pass


def sync_folders_to_sandbox(sandbox) -> None:
    """å°†æœ¬åœ° skillsã€dataã€memories æ–‡ä»¶å¤¹åŒæ­¥åˆ°æ²™ç®±ã€‚

    æ— è®ºæ²™ç®±æ˜¯æ–°å»ºçš„è¿˜æ˜¯å·²å­˜åœ¨çš„ï¼Œéƒ½æ‰§è¡ŒåŒæ­¥ä»¥ç¡®ä¿æ²™ç®±ä¸­çš„æ–‡ä»¶æ˜¯æœ€æ–°çš„ã€‚
    """
    print("\nğŸ”„ æ­£åœ¨åŒæ­¥æœ¬åœ°æ–‡ä»¶å¤¹åˆ°æ²™ç®±...")

    # ä¸Šä¼  skills æ–‡ä»¶å¤¹
    upload_skills_to_sandbox(sandbox, SKILLS_DIR, "/home/daytona/skills")

    # ä¸Šä¼  data æ–‡ä»¶å¤¹
    upload_directory_to_sandbox(
        sandbox,
        DATA_DIR,
        "/home/daytona/data",
        "data",
    )

    # ä¸Šä¼  memories æ–‡ä»¶å¤¹åˆ°æ²™ç®±ï¼ˆä¾› MemoryMiddleware ä½¿ç”¨ï¼‰
    upload_directory_to_sandbox(
        sandbox,
        MEMORIES_DIR,
        "/home/daytona/memories",
        "memories",
    )

    # éªŒè¯ä¸Šä¼ 
    print("\nğŸ” éªŒè¯åŒæ­¥çš„æ–‡ä»¶...")
    ls_result = sandbox.process.exec("find /home/daytona/skills -type f | head -10")
    print(f"æ²™ç®±ä¸­çš„ skills æ–‡ä»¶:\n{ls_result.result}")

    # éªŒè¯ data æ–‡ä»¶å¤¹å­˜åœ¨ä¸”åŒ…å« customer_db.csv
    ls_data_result = sandbox.process.exec("ls -la /home/daytona/data")
    print(f"\næ²™ç®±ä¸­çš„ data æ–‡ä»¶:\n{ls_data_result.result}")

    # æ£€æŸ¥ customer_db.csv æ˜¯å¦å­˜åœ¨
    check_csv = sandbox.process.exec(
        "test -f /home/daytona/data/customer_db.csv && echo 'âœ… customer_db.csv å­˜åœ¨' || echo 'âŒ customer_db.csv ä¸å­˜åœ¨'"
    )
    print(f"\n{check_csv.result}")

    # éªŒè¯ memories æ–‡ä»¶å¤¹å­˜åœ¨
    ls_memories_result = sandbox.process.exec("ls -la /home/daytona/memories")
    print(f"\næ²™ç®±ä¸­çš„ memories æ–‡ä»¶:\n{ls_memories_result.result}")

    # æ£€æŸ¥ MEMORY.md æ˜¯å¦å­˜åœ¨
    check_memory = sandbox.process.exec(
        "test -f /home/daytona/memories/MEMORY.md && echo 'âœ… MEMORY.md å­˜åœ¨' || echo 'âŒ MEMORY.md ä¸å­˜åœ¨'"
    )
    print(f"\n{check_memory.result}")

    print("âœ… æ–‡ä»¶å¤¹åŒæ­¥å®Œæˆ")
    print("âœ… æ–‡ä»¶å¤¹åŒæ­¥å®Œæˆ")


class SandboxSyncHandler(FileSystemEventHandler):
    """æ–‡ä»¶å˜æ›´ç›‘å¬å™¨ï¼Œè‡ªåŠ¨åŒæ­¥åˆ° Daytona æ²™ç®±ã€‚

    ä½¿ç”¨é˜²æŠ–æœºåˆ¶é¿å…é¢‘ç¹å˜æ›´å¯¼è‡´é¢‘ç¹åŒæ­¥ã€‚
    """

    def __init__(self, debounce_seconds: float = 2.0):
        """
        å‚æ•°:
            debounce_seconds: é˜²æŠ–æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œåœ¨æ­¤æ—¶é—´å†…å¤šæ¬¡å˜æ›´åªè§¦å‘ä¸€æ¬¡åŒæ­¥
        """
        self.debounce_seconds = debounce_seconds
        super().__init__()

    def on_any_event(self, event):
        """å¤„ç†ä»»ä½•æ–‡ä»¶ç³»ç»Ÿäº‹ä»¶ã€‚"""
        # å¿½ç•¥ç›®å½•äº‹ä»¶å’Œéšè—æ–‡ä»¶
        if event.is_directory:
            return

        # è·å–æ–‡ä»¶å
        file_path = Path(event.src_path)

        # å¿½ç•¥éšè—æ–‡ä»¶å’Œä¸´æ—¶æ–‡ä»¶
        if file_path.name.startswith(".") or file_path.name.endswith("~"):
            return

        # å¿½ç•¥ç‰¹å®šç›®å½•å’Œæ–‡ä»¶ç±»å‹
        if self._should_ignore(file_path):
            return

        print(f"ğŸ“ æ£€æµ‹åˆ°æ–‡ä»¶å˜æ›´: {event.event_type} - {file_path}")
        self._trigger_sync()

    def _should_ignore(self, file_path: Path) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥å¿½ç•¥è¯¥æ–‡ä»¶ã€‚"""
        # å¿½ç•¥ __pycache__ã€.git ç­‰ç›®å½•
        ignore_dirs = {
            "__pycache__",
            ".git",
            ".venv",
            "venv",
            "node_modules",
            ".pytest_cache",
        }
        for part in file_path.parts:
            if part in ignore_dirs:
                return True

        # å¿½ç•¥ç‰¹å®šæ‰©å±•å
        ignore_extensions = {".pyc", ".pyo", ".pyd", ".so", ".dylib"}
        if file_path.suffix in ignore_extensions:
            return True

        return False

    def _trigger_sync(self):
        """è§¦å‘åŒæ­¥ï¼ˆå¸¦é˜²æŠ–ï¼‰ã€‚"""
        global SYNC_DEBOUNCE_TIMER

        with SYNC_LOCK:
            # å–æ¶ˆä¹‹å‰çš„å®šæ—¶å™¨
            if SYNC_DEBOUNCE_TIMER is not None:
                SYNC_DEBOUNCE_TIMER.cancel()

            # åˆ›å»ºæ–°çš„å®šæ—¶å™¨
            SYNC_DEBOUNCE_TIMER = threading.Timer(self.debounce_seconds, self._do_sync)
            SYNC_DEBOUNCE_TIMER.start()

    def _do_sync(self):
        """æ‰§è¡Œå®é™…åŒæ­¥ã€‚"""
        global DAYTONA_SANDBOX, SYNC_DEBOUNCE_TIMER

        try:
            if DAYTONA_SANDBOX is not None:
                print("\nğŸ”„ [è‡ªåŠ¨åŒæ­¥] æ£€æµ‹åˆ°æ–‡ä»¶å˜æ›´ï¼Œæ­£åœ¨åŒæ­¥åˆ°æ²™ç®±...")
                sync_folders_to_sandbox(DAYTONA_SANDBOX)
                print("âœ… [è‡ªåŠ¨åŒæ­¥] å®Œæˆ\n")
        except Exception as e:
            print(f"âš ï¸  [è‡ªåŠ¨åŒæ­¥] å¤±è´¥: {e}")
        finally:
            with SYNC_LOCK:
                SYNC_DEBOUNCE_TIMER = None


def start_file_watcher() -> None:
    """å¯åŠ¨æ–‡ä»¶ç›‘å¬å™¨ï¼Œç›‘è§† skillsã€dataã€memories æ–‡ä»¶å¤¹ã€‚

    ä½¿ç”¨ç¯å¢ƒå˜é‡ FILE_WATCH_ENABLED æ§åˆ¶æ˜¯å¦å¯ç”¨ï¼ˆé»˜è®¤å¯ç”¨ï¼‰ã€‚
    ä½¿ç”¨ç¯å¢ƒå˜é‡ FILE_WATCH_DEBOUNCE æ§åˆ¶é˜²æŠ–æ—¶é—´ï¼ˆé»˜è®¤ 2.0 ç§’ï¼‰ã€‚
    """
    global FILE_OBSERVER

    # æ£€æŸ¥æ˜¯å¦å¯ç”¨æ–‡ä»¶ç›‘å¬
    if not _env_flag("FILE_WATCH_ENABLED", default=True):
        print("ğŸ“ æ–‡ä»¶ç›‘å¬å·²ç¦ç”¨ï¼ˆè®¾ç½® FILE_WATCH_ENABLED=0 å¯ç”¨ï¼‰")
        return

    # è·å–é˜²æŠ–æ—¶é—´
    debounce_str = os.getenv("FILE_WATCH_DEBOUNCE", "2.0")
    try:
        debounce_seconds = float(debounce_str)
    except ValueError:
        debounce_seconds = 2.0

    # åˆ›å»ºç›‘å¬å™¨
    observer = Observer()
    handler = SandboxSyncHandler(debounce_seconds=debounce_seconds)

    # ç›‘è§†çš„æ–‡ä»¶å¤¹
    watch_dirs = [
        (SKILLS_DIR, "skills"),
        (DATA_DIR, "data"),
        (MEMORIES_DIR, "memories"),
    ]

    for dir_path, label in watch_dirs:
        if dir_path.exists():
            observer.schedule(handler, str(dir_path), recursive=True)
            print(f"ğŸ“ å¼€å§‹ç›‘è§† {label} æ–‡ä»¶å¤¹: {dir_path}")
        else:
            print(f"âš ï¸  {label} æ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œè·³è¿‡ç›‘è§†: {dir_path}")

    observer.start()
    FILE_OBSERVER = observer
    print(f"âœ… æ–‡ä»¶ç›‘å¬å·²å¯åŠ¨ï¼ˆé˜²æŠ–æ—¶é—´: {debounce_seconds}ç§’ï¼‰")


def stop_file_watcher() -> None:
    """åœæ­¢æ–‡ä»¶ç›‘å¬å™¨ã€‚"""
    global FILE_OBSERVER, SYNC_DEBOUNCE_TIMER

    # å–æ¶ˆå¾…æ‰§è¡Œçš„åŒæ­¥
    with SYNC_LOCK:
        if SYNC_DEBOUNCE_TIMER is not None:
            SYNC_DEBOUNCE_TIMER.cancel()
            SYNC_DEBOUNCE_TIMER = None

    # åœæ­¢è§‚å¯Ÿè€…
    if FILE_OBSERVER is not None:
        print("\nğŸ“ åœæ­¢æ–‡ä»¶ç›‘å¬...")
        FILE_OBSERVER.stop()
        FILE_OBSERVER.join()
        FILE_OBSERVER = None
        print("âœ… æ–‡ä»¶ç›‘å¬å·²åœæ­¢")


def create_daytona_backend_with_skills(ngrok_url: str | None = None):
    """åˆ›å»º Daytona Sandboxï¼Œä¸Šä¼  skills å’Œ dataï¼Œå¹¶è¿”å› backendã€‚

    å‚æ•°:
        ngrok_url: ngrok URLï¼Œç”¨äºè·å– IP ç™½åå•ã€‚å¦‚æœæä¾›ï¼Œå°†å…è®¸æ²™ç®±è®¿é—®è¯¥ IPã€‚
                 ä¾‹å¦‚: https://nell-pluteal-doria.ngrok-free.dev

    è¿”å›:
        tuple: (backend, daytona, sandbox)
    """
    global DAYTONA_INSTANCE, DAYTONA_SANDBOX

    # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ²™ç®± IDï¼ˆå…¶ä»–è¿›ç¨‹åˆ›å»ºçš„ï¼‰
    existing_sandbox_id = _load_sandbox_id()

    # åˆå§‹åŒ– Daytona
    daytona = Daytona()
    DAYTONA_INSTANCE = daytona

    # å¦‚æœæœ‰ç°æœ‰æ²™ç®± IDï¼Œå°è¯•è¿æ¥
    if existing_sandbox_id:
        try:
            print(f"ğŸ” å°è¯•è¿æ¥ç°æœ‰æ²™ç®±: {existing_sandbox_id}")
            sandbox = daytona.get(existing_sandbox_id)
            # æµ‹è¯•æ²™ç®±æ˜¯å¦å¯ç”¨
            test_result = sandbox.process.exec('echo "ping"')
            if test_result.exit_code == 0:
                print(f"âœ… æˆåŠŸè¿æ¥åˆ°ç°æœ‰æ²™ç®±: {sandbox.id}")
                DAYTONA_SANDBOX = sandbox

                # åŒæ­¥æ–‡ä»¶å¤¹åˆ°æ²™ç®±ï¼ˆç¡®ä¿æœ¬åœ°æ›´æ–°è¢«åŒæ­¥ï¼‰
                sync_folders_to_sandbox(sandbox)

                backend = DaytonaSandbox(sandbox=sandbox)
                return backend, daytona, sandbox
            else:
                print("âš ï¸  ç°æœ‰æ²™ç®±ä¸å¯ç”¨ï¼Œåˆ›å»ºæ–°æ²™ç®±")
        except Exception as e:
            print(f"âš ï¸  è¿æ¥ç°æœ‰æ²™ç®±å¤±è´¥: {e}ï¼Œå°†åˆ›å»ºæ–°æ²™ç®±")

    print("ğŸš€ åˆ›å»ºæ–°çš„ Daytona æ²™ç®±...")

    # å‡†å¤‡æ²™ç®±å‚æ•° - è®¾ç½®è‡ªåŠ¨åœæ­¢é—´éš”ä¸º Noneï¼ˆæ°¸ä¸è‡ªåŠ¨åœæ­¢ï¼‰
    params = CreateSandboxBaseParams(
        auto_stop_interval=None,  # None = æ°¸ä¸è‡ªåŠ¨åœæ­¢
    )

    # åˆ›å»ºæ²™ç®±
    sandbox = daytona.create(params)
    DAYTONA_SANDBOX = sandbox

    # ä¿å­˜æ²™ç®± ID
    _save_sandbox_id(sandbox.id)

    print(f"âœ… æ²™ç®±åˆ›å»ºæˆåŠŸ: {sandbox.id}")

    # åŒæ­¥æ–‡ä»¶å¤¹åˆ°æ²™ç®±
    sync_folders_to_sandbox(sandbox)

    # ä½¿ç”¨ DaytonaSandbox ä½œä¸º backend
    backend = DaytonaSandbox(sandbox=sandbox)
    print("âœ… DaytonaSandbox backend åˆ›å»ºæˆåŠŸ")

    return backend, daytona, sandbox


def cleanup_daytona():
    """æ¸…ç† Daytona æ²™ç®±ã€‚"""
    global DAYTONA_INSTANCE, DAYTONA_SANDBOX

    # å…ˆåœæ­¢æ–‡ä»¶ç›‘å¬å™¨
    stop_file_watcher()

    if DAYTONA_INSTANCE and DAYTONA_SANDBOX:
        print("\nğŸ§¹ æ¸…ç† Daytona æ²™ç®±...")
        try:
            DAYTONA_INSTANCE.delete(DAYTONA_SANDBOX)
            print("âœ… æ²™ç®±å·²åˆ é™¤")
        except Exception as e:
            print(f"âš ï¸  åˆ é™¤æ²™ç®±æ—¶å‡ºé”™: {e}")
    # æ¸…é™¤æ²™ç®± ID æ–‡ä»¶
    _clear_sandbox_id()


MEMORY_AGENT_ENABLED = _env_flag("MEMORY_AGENT_ENABLED", default=True)
OUTPUT_SANITIZE_ENABLED = _env_flag("OUTPUT_SANITIZE_ENABLED", default=True)
CHECKPOINTER = MemorySaver()


def _ensure_memory_files(today: date) -> tuple[str, str]:
    MEMORIES_DIR.mkdir(parents=True, exist_ok=True)
    DAILY_DIR.mkdir(parents=True, exist_ok=True)

    today_name = today.strftime("%Y-%m-%d")
    today_file = DAILY_DIR / f"{today_name}.md"

    if not LONG_TERM_FILE.exists():
        LONG_TERM_FILE.write_text(
            "# é•¿æœŸè®°å¿†\n\n"
            "## ç”¨æˆ·åå¥½\n"
            "- æš‚æ— \n\n"
            "## é‡è¦å†³ç­–\n"
            "- æš‚æ— \n\n"
            "## å…³é”®è”ç³»äºº\n"
            "- æš‚æ— \n\n"
            "## é¡¹ç›®äº‹å®\n"
            "- æš‚æ— \n",
            encoding="utf-8",
        )

    if not today_file.exists():
        today_file.write_text(
            f"# {today_name}\n\n"
            "## 09:00 - ä¼šè¯åˆå§‹åŒ–\n"
            "- æ–°çš„ä¸€å¤©å¼€å§‹ï¼ŒæŒ‰éœ€è®°å½•é‡è¦äº‹å®ã€å†³ç­–ã€åå¥½ä¸å¾…åŠã€‚\n",
            encoding="utf-8",
        )

    return (
        "/memories/MEMORY.md",
        f"/memories/daily/{today_name}.md",
    )


def build_agent() -> Any:
    # api_key = os.getenv("OPENROUTER_API_KEY")
    api_key = os.getenv("GLM_API_KEY")
    # if not api_key:
    #     raise RuntimeError("OPENROUTER_API_KEY is missing. Please set it in .env")
    #
    if not api_key:
        raise RuntimeError("GLM_API_KEY is missing. Please set it in .env")

    os.environ["OPENAI_API_KEY"] = api_key
    # llm = ChatOpenAI(
    #     model=model_name,
    #     base_url="https://openrouter.ai/api/v1",
    #     # temperature=0.2,
    # )
    llm = ChatOpenAI(
        model=model_name,
        base_url="https://open.bigmodel.cn/api/paas/v4",
        temperature=0.1,
    )

    today = date.today()
    yesterday = today - timedelta(days=1)
    long_term_path, today_path = _ensure_memory_files(today)
    yesterday_path = f"/memories/daily/{yesterday.strftime('%Y-%m-%d')}.md"

    system_prompt = PROMPTS.render_prompt(
        "main_system",
        {
            "long_term_path": str(long_term_path),
            "today_path": str(today_path),
            "yesterday_path": str(yesterday_path),
        },
    )

    # ä½¿ç”¨ Daytona Sandbox ä½œä¸º backendï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if DAYTONA_AVAILABLE:
        print("ğŸš€ ä½¿ç”¨ Daytona Sandbox backend")
        backend, _, _ = create_daytona_backend_with_skills()
        # skills ä½¿ç”¨æ²™ç®±ä¸­çš„è·¯å¾„
        skills = ["/home/daytona/skills"]
        # è®°å¿†æ–‡ä»¶ä½¿ç”¨æ²™ç®±ä¸­çš„è·¯å¾„
        memory_paths = [
            "/home/daytona/memories/MEMORY.md",
            f"/home/daytona/memories/daily/{today.strftime('%Y-%m-%d')}.md",
            f"/home/daytona/memories/daily/{yesterday.strftime('%Y-%m-%d')}.md",
        ]
    else:
        print("âš ï¸  Daytona ä¸å¯ç”¨ï¼Œä½¿ç”¨æœ¬åœ° FilesystemBackend")
        from deepagents.backends import FilesystemBackend

        backend = FilesystemBackend(root_dir=str(PROJECT_ROOT))
        skills = [str(SKILLS_DIR)]
        # è®°å¿†æ–‡ä»¶ä½¿ç”¨æœ¬åœ°è™šæ‹Ÿè·¯å¾„
        memory_paths = [long_term_path, today_path, yesterday_path]
    agent = create_deep_agent(
        model=llm,
        store=InMemoryStore(),
        backend=backend,
        skills=skills,
        memory=memory_paths,
        checkpointer=CHECKPOINTER,
        system_prompt=system_prompt,
    )

    return agent


def build_memory_agent(backend: Any | None = None) -> Any:
    """æ„å»º Memory Agentã€‚

    Args:
        backend: å¯é€‰çš„ backend å®ä¾‹ã€‚å¦‚æœæä¾›ï¼Œå°†ä½¿ç”¨è¯¥ backend è€Œä¸æ˜¯åˆ›å»ºæ–°çš„ã€‚
                 è¿™æ ·å¯ä»¥ç¡®ä¿ Memory Agent å’Œä¸» Agent è®¿é—®ç›¸åŒçš„æ–‡ä»¶ç³»ç»Ÿã€‚
    """
    api_key = os.getenv("OPENROUTER_API_KEY")
    if not api_key:
        raise RuntimeError("OPENROUTER_API_KEY is missing. Please set it in .env")

    os.environ["OPENAI_API_KEY"] = api_key
    llm = ChatOpenAI(
        model=memory_model_name,
        base_url="https://openrouter.ai/api/v1",
        temperature=0.1,
    )

    today = date.today()
    yesterday = today - timedelta(days=1)
    long_term_path, today_path = _ensure_memory_files(today)
    yesterday_path = f"/memories/daily/{yesterday.strftime('%Y-%m-%d')}.md"

    # æ„å»ºè®°å¿†è·¯å¾„åˆ—è¡¨ï¼ˆä¸ä¸» Agent ç›¸åŒï¼‰
    if DAYTONA_AVAILABLE and backend is not None:
        # ä½¿ç”¨ä¸ä¸» Agent ç›¸åŒçš„ Daytona backend
        memory_paths = [
            "/home/daytona/memories/MEMORY.md",
            f"/home/daytona/memories/daily/{today.strftime('%Y-%m-%d')}.md",
            f"/home/daytona/memories/daily/{yesterday.strftime('%Y-%m-%d')}.md",
        ]
    else:
        # ä½¿ç”¨æœ¬åœ° FilesystemBackend
        memory_paths = [long_term_path, today_path, yesterday_path]

    # å¦‚æœæ²¡æœ‰æä¾› backendï¼Œåˆ›å»ºé»˜è®¤çš„æœ¬åœ° backend
    if backend is None:
        backend = FilesystemBackend(root_dir=str(PROJECT_ROOT), virtual_mode=True)

    system_prompt = PROMPTS.render_prompt("memory_agent")
    agent = create_deep_agent(
        model=llm,
        backend=backend,
        memory=memory_paths,
        system_prompt=system_prompt,
    )
    return agent


def _list_skill_packages(skills_dir: Path) -> list[dict[str, str]]:
    if not skills_dir.exists():
        return []

    seen: set[str] = set()
    packages: list[dict[str, str]] = []
    for skill_file in skills_dir.rglob("SKILL.md"):
        parent_dir = skill_file.parent
        try:
            rel = parent_dir.relative_to(skills_dir).as_posix()
        except ValueError:
            continue

        if rel in seen:
            continue

        seen.add(rel)
        packages.append({"name": parent_dir.name, "path": f"/skills/{rel}"})

    packages.sort(key=lambda item: (item["name"], item["path"]))
    return packages


def _customers_payload(csv_path: Path) -> list[dict[str, str]]:
    if not csv_path.exists() or not csv_path.is_file():
        return []

    customers: list[dict[str, str]] = []
    with csv_path.open("r", encoding="utf-8", newline="") as handle:
        reader = csv.DictReader(handle)
        for row in reader:
            if not isinstance(row, dict):
                continue

            name = str(row.get("name", "")).strip()
            age = str(row.get("age", "")).strip()
            gender = str(row.get("gender", "")).strip()
            behavior = str(row.get("behaviors", "")).strip()
            if not name:
                continue

            customers.append(
                {
                    "name": name,
                    "age": age,
                    "gender": gender,
                    "behavior": behavior,
                }
            )

    return customers


def _frontend_asset_version() -> str:
    candidates = [
        FRONTEND_DIR / "index.html",
        FRONTEND_DIR / "style.css",
        FRONTEND_DIR / "script.js",
    ]
    latest_mtime = 0.0
    for path in candidates:
        if path.exists():
            latest_mtime = max(latest_mtime, path.stat().st_mtime)

    return str(int(latest_mtime)) if latest_mtime else "1"


def _latest_daily_memory_files(limit: int = 2) -> list[Path]:
    if not DAILY_DIR.exists():
        return []

    files = [p for p in DAILY_DIR.glob("*.md") if p.is_file()]
    files.sort(key=lambda p: p.name, reverse=True)
    return files[:limit]


def _read_text_file(path: Path) -> str:
    if not path.exists() or not path.is_file():
        return ""
    return path.read_text(encoding="utf-8", errors="replace")


def _memories_payload() -> dict[str, Any]:
    today = date.today()
    _ensure_memory_files(today)

    short_files = _latest_daily_memory_files(limit=2)
    short_term: list[dict[str, str]] = []
    max_mtime = LONG_TERM_FILE.stat().st_mtime if LONG_TERM_FILE.exists() else 0.0

    for file_path in short_files:
        short_term.append(
            {
                "date": file_path.stem,
                "path": f"/memories/daily/{file_path.name}",
                "content": _read_text_file(file_path),
            }
        )
        max_mtime = max(max_mtime, file_path.stat().st_mtime)

    long_term_content = _read_text_file(LONG_TERM_FILE)
    if LONG_TERM_FILE.exists():
        max_mtime = max(max_mtime, LONG_TERM_FILE.stat().st_mtime)

    return {
        "version": str(int(max_mtime)) if max_mtime else "1",
        "short_term": short_term,
        "long_term": {
            "path": "/memories/MEMORY.md",
            "content": long_term_content,
        },
    }


# é¢„åŠ è½½çš„ Agent å®ä¾‹ï¼ˆåº”ç”¨å¯åŠ¨æ—¶åˆå§‹åŒ–ï¼‰
AGENT: Any | None = None
MEMORY_AGENT: Any | None = None

# ä½¿ç”¨æ–‡ä»¶é”ç¡®ä¿å¤šè¿›ç¨‹å®‰å…¨åˆå§‹åŒ–
_AGENTS_INITIALIZED = False
_INIT_LOCK_FILE = Path(__file__).resolve().parent / ".agents_init.lock"
_INIT_LOCK_FD: int | None = None


def _acquire_flock() -> bool:
    """è·å–æ–‡ä»¶é”ï¼ˆéé˜»å¡ï¼‰ã€‚

    Returns:
        True: æˆåŠŸè·å–é”
        False: é”å·²è¢«å…¶ä»–è¿›ç¨‹æŒæœ‰
    """
    global _INIT_LOCK_FD
    try:
        import fcntl

        _INIT_LOCK_FD = os.open(str(_INIT_LOCK_FILE), os.O_CREAT | os.O_RDWR)
        # å°è¯•è·å–éé˜»å¡æ’ä»–é”
        fcntl.flock(_INIT_LOCK_FD, fcntl.LOCK_EX | fcntl.LOCK_NB)
        # å†™å…¥å½“å‰ PID
        os.ftruncate(_INIT_LOCK_FD, 0)
        os.write(_INIT_LOCK_FD, str(os.getpid()).encode())
        os.fsync(_INIT_LOCK_FD)
        return True
    except (OSError, IOError, ImportError):
        # è·å–é”å¤±è´¥ï¼ˆè¢«å…¶ä»–è¿›ç¨‹æŒæœ‰ï¼‰
        if _INIT_LOCK_FD is not None:
            try:
                os.close(_INIT_LOCK_FD)
            except:
                pass
            _INIT_LOCK_FD = None
        return False


def _release_flock() -> None:
    """é‡Šæ”¾æ–‡ä»¶é”ã€‚"""
    global _INIT_LOCK_FD
    if _INIT_LOCK_FD is not None:
        try:
            import fcntl

            fcntl.flock(_INIT_LOCK_FD, fcntl.LOCK_UN)
            os.close(_INIT_LOCK_FD)
        except:
            pass
        finally:
            _INIT_LOCK_FD = None
    # å°è¯•åˆ é™¤é”æ–‡ä»¶
    try:
        if _INIT_LOCK_FILE.exists():
            _INIT_LOCK_FILE.unlink()
    except:
        pass


def _release_flock_keep_file() -> None:
    """é‡Šæ”¾æ–‡ä»¶é”ä½†ä¿ç•™é”æ–‡ä»¶ä½œä¸º'å·²åˆå§‹åŒ–'æ ‡è®°ã€‚"""
    global _INIT_LOCK_FD
    if _INIT_LOCK_FD is not None:
        try:
            import fcntl

            fcntl.flock(_INIT_LOCK_FD, fcntl.LOCK_UN)
            os.close(_INIT_LOCK_FD)
        except:
            pass
        finally:
            _INIT_LOCK_FD = None


def _is_uvicorn_reloader_process() -> bool:
    """æ£€æµ‹å½“å‰æ˜¯å¦æ˜¯ Uvicorn çš„ reloader è¿›ç¨‹ã€‚"""
    # Uvicorn reloader è¿›ç¨‹ä¼šè®¾ç½®ç‰¹æ®Šçš„ç¯å¢ƒå˜é‡
    if sys.argv:
        return (
            os.getenv("UVICORN_RELOADER") == "1" or "uvicorn.supervisors" in sys.argv[0]
        )
    return False


def _is_worker_process() -> bool:
    """æ£€æµ‹å½“å‰æ˜¯å¦æ˜¯ Uvicorn worker è¿›ç¨‹ï¼ˆé reloaderï¼‰ã€‚"""
    # æ£€æŸ¥æ˜¯å¦æœ‰ Uvicorn ç‰¹å®šçš„ç¯å¢ƒå˜é‡
    return os.getenv("UVICORN_WORKER") == "1" or os.getenv("GUNICORN_WORKER") == "1"


def init_agents() -> None:
    """åœ¨åº”ç”¨å¯åŠ¨æ—¶é¢„åŠ è½½æ‰€æœ‰ Agent å’Œæ²™ç®±ã€‚

    ä½¿ç”¨æ–‡ä»¶é”ç¡®ä¿å¤šè¿›ç¨‹å®‰å…¨ï¼šåªæœ‰ä¸€ä¸ªè¿›ç¨‹èƒ½æˆåŠŸåˆå§‹åŒ–ï¼Œ
    å…¶ä»–è¿›ç¨‹ä¼šç­‰å¾…åˆå§‹åŒ–å®Œæˆåç›´æ¥è¿”å›ã€‚

    æ³¨æ„ï¼šæ­¤å‡½æ•°åº”è¯¥åœ¨ main() ä¸­è°ƒç”¨ï¼Œä¸è¦åœ¨æ¨¡å—å¯¼å…¥æ—¶è°ƒç”¨ã€‚
    """
    global AGENT, MEMORY_AGENT, _AGENTS_INITIALIZED

    # å¿«é€Ÿæ£€æŸ¥ï¼šå¦‚æœå·²åˆå§‹åŒ–ï¼Œç›´æ¥è¿”å›
    if _AGENTS_INITIALIZED and AGENT is not None:
        print("âš ï¸  Agents å·²åˆå§‹åŒ–ï¼Œè·³è¿‡é‡å¤åˆå§‹åŒ–")
        return

    # å°è¯•è·å–æ–‡ä»¶é”
    if not _acquire_flock():
        # è·å–é”å¤±è´¥ï¼Œè¯´æ˜å…¶ä»–è¿›ç¨‹æ­£åœ¨åˆå§‹åŒ–
        print("â³ æ£€æµ‹åˆ°å…¶ä»–è¿›ç¨‹æ­£åœ¨åˆå§‹åŒ– Agentsï¼Œç­‰å¾…ä¸­...")

        # ç­‰å¾…å…¶ä»–è¿›ç¨‹å®Œæˆåˆå§‹åŒ–ï¼ˆé€šè¿‡è½®è¯¢æ£€æŸ¥ AGENT æ˜¯å¦è¢«è®¾ç½®ï¼‰
        timeout = 60.0
        start = time.time()
        while time.time() - start < timeout:
            if _AGENTS_INITIALIZED or AGENT is not None:
                print("âœ… å…¶ä»–è¿›ç¨‹åˆå§‹åŒ–å®Œæˆï¼Œå¤ç”¨å·²åˆ›å»ºçš„ Agents")
                return
            time.sleep(0.5)

        print("âš ï¸  ç­‰å¾…è¶…æ—¶ï¼Œå°è¯•å¼ºåˆ¶æ¥ç®¡åˆå§‹åŒ–...")
        # é‡Šæ”¾æ—§é”ï¼ˆå¦‚æœæœ‰ï¼‰å¹¶é‡æ–°å°è¯•
        _release_flock()
        if not _acquire_flock():
            print("âŒ æ— æ³•è·å–åˆå§‹åŒ–é”ï¼Œè·³è¿‡åˆå§‹åŒ–")
            return

    # è·å–é”æˆåŠŸï¼Œå¼€å§‹åˆå§‹åŒ–
    try:
        # åŒé‡æ£€æŸ¥ï¼šè·å–é”åå†æ¬¡ç¡®è®¤æ˜¯å¦å·²åˆå§‹åŒ–
        if _AGENTS_INITIALIZED and AGENT is not None:
            print("âš ï¸  Agents å·²åœ¨å…¶ä»–è¿›ç¨‹ä¸­åˆå§‹åŒ–ï¼Œè·³è¿‡")
            return

        init_start_time = time.time()

        print("=" * 60)
        print("ğŸš€ é¢„åŠ è½½ Agents...")
        print("=" * 60)

        # é¢„åŠ è½½ä¸» Agentï¼ˆä¼šåˆ›å»º Daytona æ²™ç®±ï¼‰
        print("\n[1/2] åˆå§‹åŒ–ä¸» Agent (Daytona Sandbox)...")
        AGENT = build_agent()
        print("âœ… ä¸» Agent åˆå§‹åŒ–å®Œæˆ")

        # é¢„åŠ è½½ Memory Agentï¼ˆä½¿ç”¨ä¸ä¸» Agent ç›¸åŒçš„ backendï¼‰
        if MEMORY_AGENT_ENABLED:
            print("\n[2/2] åˆå§‹åŒ– Memory Agent...")
            # è·å–ä¸» Agent çš„ backend
            main_agent_backend = None
            if hasattr(AGENT, "_backend"):
                main_agent_backend = AGENT._backend
            elif hasattr(AGENT, "backend"):
                main_agent_backend = AGENT.backend

            MEMORY_AGENT = build_memory_agent(backend=main_agent_backend)
            print("âœ… Memory Agent åˆå§‹åŒ–å®Œæˆ")
        else:
            print("\n[2/2] Memory Agent å·²ç¦ç”¨ï¼Œè·³è¿‡åˆå§‹åŒ–")

        _AGENTS_INITIALIZED = True
        elapsed = time.time() - init_start_time
        print("\n" + "=" * 60)
        print(f"âœ… æ‰€æœ‰ Agents é¢„åŠ è½½å®Œæˆï¼({elapsed:.1f}s)")
        print("=" * 60 + "\n")

        # å¯åŠ¨æ–‡ä»¶ç›‘å¬å™¨ï¼ˆè‡ªåŠ¨åŒæ­¥æœ¬åœ°å˜æ›´åˆ°æ²™ç®±ï¼‰
        start_file_watcher()

    except Exception as e:
        print(f"âŒ åˆå§‹åŒ– Agents å¤±è´¥: {e}")
        raise
    finally:
        # é‡Šæ”¾é”ï¼Œä½†ä¿ç•™æ–‡ä»¶ä½œä¸º"å·²åˆå§‹åŒ–"æ ‡è®°
        # è¿™æ ·å…¶ä»–è¿›ç¨‹å¯ä»¥æ£€æµ‹åˆ°åˆå§‹åŒ–å·²å®Œæˆ
        time.sleep(1)
        _release_flock_keep_file()


def get_agent() -> Any:
    """è·å–å·²é¢„åŠ è½½çš„ä¸» Agentã€‚"""
    if AGENT is None:
        raise RuntimeError("Agent å°šæœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨ init_agents()")
    return AGENT


def get_memory_agent() -> Any:
    """è·å–å·²é¢„åŠ è½½çš„ Memory Agentã€‚"""
    if MEMORY_AGENT is None:
        raise RuntimeError("Memory Agent å°šæœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨ init_agents()")
    return MEMORY_AGENT


def _now_hhmm() -> str:
    return datetime.now().strftime("%H:%M")


def _append_daily_memory(title: str, bullets: list[str], today_path: Path) -> None:
    cleaned_title = title.strip() or "ä¼šè¯è®°å½•"
    cleaned_bullets = [x.strip() for x in bullets if x and x.strip()]
    if not cleaned_bullets:
        return

    block_lines = [f"## {_now_hhmm()} - {cleaned_title}"]
    for line in cleaned_bullets:
        block_lines.append(f"- {line}")
    block = "\n" + "\n".join(block_lines) + "\n"

    current = _read_text_file(today_path)
    today_path.write_text(current + block, encoding="utf-8")


def _parse_long_term_sections(content: str) -> dict[str, list[str]]:
    section_names = ["ç”¨æˆ·åå¥½", "é‡è¦å†³ç­–", "å…³é”®è”ç³»äºº", "é¡¹ç›®äº‹å®"]
    sections: dict[str, list[str]] = {name: [] for name in section_names}
    current_section = ""

    for raw in content.splitlines():
        line = raw.strip()
        if line.startswith("## "):
            name = line[3:].strip()
            current_section = name if name in sections else ""
            continue
        if current_section and line.startswith("-"):
            item = line[1:].strip()
            if item and item != "æš‚æ— ":
                sections[current_section].append(item)

    return sections


def _render_long_term_sections(sections: dict[str, list[str]]) -> str:
    ordered_names = ["ç”¨æˆ·åå¥½", "é‡è¦å†³ç­–", "å…³é”®è”ç³»äºº", "é¡¹ç›®äº‹å®"]
    lines = ["# é•¿æœŸè®°å¿†", ""]
    for name in ordered_names:
        lines.append(f"## {name}")
        items = []
        seen: set[str] = set()
        for text in sections.get(name, []):
            normalized = text.strip()
            if not normalized or normalized in seen:
                continue
            seen.add(normalized)
            items.append(normalized)
        if not items:
            lines.append("- æš‚æ— ")
        else:
            for item in items:
                lines.append(f"- {item}")
        lines.append("")
    return "\n".join(lines).rstrip() + "\n"


def _merge_long_term_memory(
    updates: dict[str, list[str]], long_term_path: Path
) -> None:
    current = _read_text_file(long_term_path)
    sections = _parse_long_term_sections(current)

    for key, values in updates.items():
        if key not in sections or not isinstance(values, list):
            continue
        for value in values:
            if isinstance(value, str) and value.strip():
                sections[key].append(value.strip())

    long_term_path.write_text(_render_long_term_sections(sections), encoding="utf-8")


def _is_internal_leak_line(line: str) -> bool:
    if not OUTPUT_SANITIZE_ENABLED or not OUTPUT_SANITIZE_CONFIG.enabled:
        return False

    s = line.strip()
    if not s:
        return False

    if any(token in s for token in OUTPUT_SANITIZE_CONFIG.literals):
        return True

    return any(pattern.search(s) for pattern in OUTPUT_SANITIZE_CONFIG.regex_patterns)


def _sanitize_user_facing_text(text: str) -> str:
    """æ¸…ç†ç”¨æˆ·å¯è§æ–‡æœ¬ï¼Œåªè¿‡æ»¤å†…éƒ¨ä¿¡æ¯æ³„æ¼ï¼Œä¿ç•™ Markdown æ ¼å¼ã€‚"""
    lines = text.splitlines()
    kept = [line for line in lines if not _is_internal_leak_line(line)]
    cleaned = "\n".join(kept).strip()
    return cleaned


def _consume_stream_buffer(buffer: str) -> tuple[str, str]:
    if "\n" not in buffer:
        return "", buffer

    parts = buffer.split("\n")
    remain = parts.pop() if parts else ""
    output_parts: list[str] = []
    for raw_line in parts:
        # åªè¿‡æ»¤å†…éƒ¨ä¿¡æ¯æ³„æ¼ï¼Œä¿ç•™ Markdown æ ¼å¼
        if _is_internal_leak_line(raw_line):
            continue
        output_parts.append(raw_line)

    flushed = "\n".join(output_parts)
    if flushed:
        flushed += "\n"
    return flushed, remain


def _run_memory_agent_sync(user_message: str, assistant_message: str) -> None:
    if not user_message.strip() or not assistant_message.strip():
        return

    with MEMORY_WRITE_LOCK:
        today = date.today()
        _, today_virtual_path = _ensure_memory_files(today)
        today_path = PROJECT_ROOT / today_virtual_path.lstrip("/")
        yesterday = today - timedelta(days=1)
        yesterday_virtual_path = f"/memories/daily/{yesterday.strftime('%Y-%m-%d')}.md"
        yesterday_path = PROJECT_ROOT / yesterday_virtual_path.lstrip("/")

        long_term_path = LONG_TERM_FILE
        today_log_content = _read_text_file(today_path)
        yesterday_log_content = _read_text_file(yesterday_path)
        long_term_content = _read_text_file(long_term_path)

        memory_task = (
            "è¯·åŸºäºä»¥ä¸‹æœ¬è½®ä¼šè¯æ›´æ–°è®°å¿†æ–‡ä»¶ï¼Œå¹¶ç›´æ¥ä½¿ç”¨æ–‡ä»¶å·¥å…·è½ç›˜ã€‚\n\n"
            f"ç”¨æˆ·æ¶ˆæ¯:\n{user_message}\n\n"
            f"åŠ©æ‰‹å›å¤:\n{assistant_message}\n\n"
            f"ä»Šæ—¥æ—¥å¿—è·¯å¾„: {today_virtual_path}\n"
            f"æ˜¨æ—¥æ—¥å¿—è·¯å¾„: {yesterday_virtual_path}\n"
            "é•¿æœŸè®°å¿†è·¯å¾„: /memories/MEMORY.md\n\n"
            "ä»Šå¤©æ—¥å¿—å½“å‰å†…å®¹:\n"
            f"{today_log_content or '(ç©º)'}\n\n"
            "æ˜¨å¤©æ—¥å¿—å½“å‰å†…å®¹:\n"
            f"{yesterday_log_content or '(ç©º)'}\n\n"
            "é•¿æœŸè®°å¿†å½“å‰å†…å®¹:\n"
            f"{long_term_content or '(ç©º)'}\n"
        )

        get_memory_agent().invoke(
            {"messages": [{"role": "user", "content": memory_task}]},
            config={
                "configurable": {"thread_id": f"memory-{today.strftime('%Y-%m-%d')}"}
            },
        )


def _dispatch_memory_agent(user_message: str, assistant_message: str) -> None:
    if not MEMORY_AGENT_ENABLED:
        return

    worker = threading.Thread(
        target=_run_memory_agent_sync,
        args=(user_message, assistant_message),
        daemon=True,
    )
    worker.start()


def _truncate_history(history: list[Any], max_rounds: int = 3) -> list[Any]:
    """æˆªæ–­å†å²å¯¹è¯ï¼Œåªä¿ç•™æœ€è¿‘ N è½® + å½“å‰è½®ã€‚

    Args:
        history: å®Œæ•´çš„å†å²å¯¹è¯åˆ—è¡¨
        max_rounds: ä¿ç•™çš„å¯¹è¯è½®æ•°ï¼ˆé»˜è®¤ 3 è½®ï¼‰

    Returns:
        æˆªæ–­åçš„å†å²åˆ—è¡¨
    """
    if not history:
        return []

    # è¿‡æ»¤å‡ºæœ‰æ•ˆçš„å¯¹è¯è½®ï¼ˆuser + assistant ä¸ºä¸€å¯¹ï¼‰
    valid_messages = []
    for item in history:
        if isinstance(item, dict):
            role = item.get("role")
            if role in {"user", "assistant"}:
                valid_messages.append(item)
        elif isinstance(item, (list, tuple)) and len(item) == 2:
            # å¤„ç† [user_msg, assistant_msg] æ ¼å¼
            user_msg, assistant_msg = item
            if user_msg:
                valid_messages.append({"role": "user", "content": str(user_msg)})
            if assistant_msg:
                valid_messages.append(
                    {"role": "assistant", "content": str(assistant_msg)}
                )

    # åªä¿ç•™æœ€è¿‘ N è½®ï¼ˆæ¯è½®åŒ…å« user + assistant ä¸¤æ¡æ¶ˆæ¯ï¼‰
    max_messages = max_rounds * 2
    truncated = (
        valid_messages[-max_messages:]
        if len(valid_messages) > max_messages
        else valid_messages
    )

    if len(valid_messages) > max_messages:
        print(
            f"ğŸ“‰ å†å²å¯¹è¯å·²æˆªæ–­: {len(valid_messages)} æ¡ -> {len(truncated)} æ¡ (ä¿ç•™æœ€è¿‘ {max_rounds} è½®)"
        )

    return truncated


def _to_deepagent_messages(history: list[Any], user_text: str) -> list[dict[str, str]]:
    """å°†å†å²å¯¹è¯è½¬æ¢ä¸º deepagent æ¶ˆæ¯æ ¼å¼ã€‚

    æ³¨æ„ï¼šè¿™é‡Œçš„å†å²ä¼šè¢«æˆªæ–­ï¼Œåªä¿ç•™æœ€è¿‘ 3 è½®ã€‚
    æ—©æœŸå¯¹è¯å†…å®¹å·²é€šè¿‡è®°å¿†ç³»ç»Ÿä¿å­˜åˆ° system promptã€‚
    """
    messages: list[dict[str, str]] = []

    # æˆªæ–­å†å²ï¼Œåªä¿ç•™æœ€è¿‘ 3 è½®
    truncated_history = _truncate_history(history, max_rounds=3)

    for item in truncated_history:
        if isinstance(item, dict):
            role = item.get("role")
            content = item.get("content", "")
            if role in {"user", "assistant"}:
                messages.append({"role": role, "content": str(content)})
            continue

        if isinstance(item, (list, tuple)) and len(item) == 2:
            user_msg, assistant_msg = item
            if user_msg:
                messages.append({"role": "user", "content": str(user_msg)})
            if assistant_msg:
                messages.append({"role": "assistant", "content": str(assistant_msg)})

    messages.append({"role": "user", "content": user_text})
    return messages


def chat_with_agent(
    message: str, history: list[Any], thread_id: str | None
) -> tuple[str, str, list[dict[str, str]]]:
    resolved_thread_id = thread_id or str(uuid.uuid4())
    input_messages = [{"role": "user", "content": message}]

    result = get_agent().invoke(
        {"messages": input_messages},
        config={"configurable": {"thread_id": resolved_thread_id}},
    )

    assistant_text = ""
    for msg in reversed(result.get("messages", [])):
        if _get_msg_type(msg) == "ai":
            assistant_text = _message_content_to_text(_get_msg_content(msg))
            break

    assistant_text = _sanitize_user_facing_text(assistant_text)

    updated_history = _to_deepagent_messages(history, message)
    updated_history.append({"role": "assistant", "content": assistant_text})
    _dispatch_memory_agent(user_message=message, assistant_message=assistant_text)
    return assistant_text, resolved_thread_id, updated_history


def _message_content_to_text(content: Any) -> str:
    if isinstance(content, str):
        return content

    if isinstance(content, list):
        parts: list[str] = []
        for item in content:
            if isinstance(item, dict):
                text = item.get("text") or item.get("content")
                if isinstance(text, str):
                    parts.append(text)
            elif hasattr(item, "text"):
                text = getattr(item, "text", "")
                if isinstance(text, str):
                    parts.append(text)
            elif hasattr(item, "content"):
                text = getattr(item, "content", "")
                if isinstance(text, str):
                    parts.append(text)
            elif isinstance(item, str):
                parts.append(item)
        return "".join(parts)

    return ""


def _get_msg_type(msg: Any) -> str:
    if isinstance(msg, dict):
        return str(msg.get("type", ""))
    return str(getattr(msg, "type", ""))


def _get_msg_content(msg: Any) -> Any:
    if isinstance(msg, dict):
        return msg.get("content", "")
    return getattr(msg, "content", "")


def _extract_ai_text_from_output(output: Any) -> str:
    if not isinstance(output, dict):
        return ""

    messages = output.get("messages")
    if not isinstance(messages, list):
        return ""

    for msg in reversed(messages):
        if _get_msg_type(msg) == "ai":
            text = _message_content_to_text(_get_msg_content(msg))
            if text:
                return text
    return ""


def _extract_event_chunk_text(chunk: Any) -> str:
    if chunk is None:
        return ""

    if hasattr(chunk, "content"):
        return _message_content_to_text(getattr(chunk, "content"))

    if isinstance(chunk, dict) and "content" in chunk:
        return _message_content_to_text(chunk.get("content"))

    return _message_content_to_text(chunk)


def _preview_output(value: Any, limit: int = 120) -> str:
    text = _message_content_to_text(value)
    if not text:
        text = str(value)
    text = text.replace("\n", " ").strip()
    if len(text) > limit:
        return f"{text[:limit]}..."
    return text


async def stream_chat_with_agent(
    message: str, history: list[Any], thread_id: str | None
) -> AsyncIterator[dict[str, Any]]:
    resolved_thread_id = thread_id or str(uuid.uuid4())
    input_messages = [{"role": "user", "content": message}]

    assistant_full_text = ""
    started_nodes: set[str] = set()
    completed_nodes: set[str] = set()
    fallback_ai_text = ""
    stream_buffer = ""

    yield {"event": "start", "thread_id": resolved_thread_id}

    try:
        event_iter = get_agent().astream_events(
            {"messages": input_messages},
            config={"configurable": {"thread_id": resolved_thread_id}},
            version="v2",
        )

        async for event in event_iter:
            kind = str(event.get("event", ""))
            name = str(event.get("name", ""))
            data = event.get("data") if isinstance(event.get("data"), dict) else {}
            metadata = (
                event.get("metadata") if isinstance(event.get("metadata"), dict) else {}
            )
            node_name = str(metadata.get("langgraph_node") or name or "")

            if kind == "on_chat_model_stream":
                chunk = data.get("chunk")
                delta = _extract_event_chunk_text(chunk)
                if delta:
                    stream_buffer += delta
                    flushed, stream_buffer = _consume_stream_buffer(stream_buffer)
                    if flushed:
                        assistant_full_text += flushed
                        yield {"event": "delta", "text": flushed}
                continue

            if kind == "on_chain_start":
                if (
                    node_name
                    and node_name not in started_nodes
                    and node_name != "agent"
                ):
                    started_nodes.add(node_name)
                    yield {"event": "process", "text": f"è¿›å…¥é˜¶æ®µ: {node_name}"}
                continue

            if kind == "on_tool_start":
                tool_input = data.get("input")
                yield {"event": "process", "text": f"è°ƒç”¨å·¥å…·: {name}"}
                if tool_input is not None:
                    yield {
                        "event": "process",
                        "text": f"å·¥å…·å‚æ•°: {_preview_output(tool_input)}",
                    }
                continue

            if kind == "on_tool_end":
                tool_output = data.get("output")
                tool_result_preview = ""
                if tool_output is not None:
                    tool_result_preview = _preview_output(tool_output)

                # æ£€æµ‹ start_countdown_async å·¥å…·å®Œæˆï¼Œå‘é€ countdown_started äº‹ä»¶
                if name == "start_countdown_async" and tool_output:
                    task_id = tool_output.get("task_id")
                    countdown_seconds = tool_output.get("countdown_seconds", 10)
                    if task_id:
                        yield {
                            "event": "countdown_started",
                            "task_id": task_id,
                            "countdown_seconds": countdown_seconds,
                        }

                yield {"event": "process", "text": f"å·¥å…·å®Œæˆ: {name}"}
                if tool_result_preview:
                    yield {
                        "event": "process",
                        "text": f"å·¥å…·ç»“æœ: {tool_result_preview}",
                    }
                continue

            if kind == "on_chain_end":
                if (
                    node_name
                    and node_name not in completed_nodes
                    and node_name != "agent"
                ):
                    completed_nodes.add(node_name)
                    yield {"event": "process", "text": f"é˜¶æ®µå®Œæˆ: {node_name}"}

                output = data.get("output")
                if not assistant_full_text and output is not None:
                    fallback = _extract_ai_text_from_output(output)
                    if fallback:
                        fallback_ai_text = fallback
    except Exception as exc:
        if assistant_full_text:
            yield {"event": "error", "detail": f"stream interrupted: {exc}"}
        else:
            yield {"event": "error", "detail": f"stream failed: {exc}"}

    if stream_buffer:
        # ä¿ç•™ Markdown æ ¼å¼ï¼Œåªè¿‡æ»¤å†…éƒ¨ä¿¡æ¯
        if not _is_internal_leak_line(stream_buffer):
            assistant_full_text += stream_buffer
            yield {"event": "delta", "text": stream_buffer}

    if not assistant_full_text and fallback_ai_text:
        assistant_full_text = _sanitize_user_facing_text(fallback_ai_text)
        if assistant_full_text:
            yield {"event": "delta", "text": assistant_full_text}

    assistant_full_text = _sanitize_user_facing_text(assistant_full_text)

    updated_history = _to_deepagent_messages(history, message)
    updated_history.append({"role": "assistant", "content": assistant_full_text})
    _dispatch_memory_agent(user_message=message, assistant_message=assistant_full_text)
    yield {
        "event": "done",
        "reply": assistant_full_text,
        "thread_id": resolved_thread_id,
        "history": updated_history,
    }


class ChatRequest(BaseModel):
    message: str
    history: list[Any] = Field(default_factory=list)
    thread_id: str | None = None


def _to_sse(event: dict[str, Any]) -> str:
    payload = json.dumps(event, ensure_ascii=False)
    return f"data: {payload}\n\n"


def create_app() -> FastAPI:
    app = FastAPI(title="AI Chat Platform", version="1.0.0")
    app.mount("/static", StaticFiles(directory=str(FRONTEND_DIR)), name="static")

    print("ğŸš€ ä¸»è¿›ç¨‹ï¼šé¢„åŠ è½½ Agents...")
    init_agents()
    print("âœ… Agents é¢„åŠ è½½å®Œæˆï¼Œå¯åŠ¨ Uvicorn...\n")

    # æ³¨å†Œä¿¡å·å¤„ç†ç¨‹åºï¼Œç¡®ä¿ç¨‹åºé€€å‡ºæ—¶æ¸…ç†æ²™ç®±
    @app.get("/api/health")
    def health() -> dict[str, str]:
        return {"status": "ok", "model": model_name}

    @app.get("/api/skills")
    def skills() -> dict[str, Any]:
        return {"skills": _list_skill_packages(PROJECT_ROOT / "skills")}

    @app.get("/api/customers")
    def customers() -> dict[str, Any]:
        return {
            "customers": _customers_payload(PROJECT_ROOT / "data" / "customer_db.csv")
        }

    @app.get("/api/memories")
    def memories() -> dict[str, Any]:
        return _memories_payload()

    @app.post("/api/sync")
    def sync_to_sandbox() -> dict[str, Any]:
        """æ‰‹åŠ¨è§¦å‘æœ¬åœ°æ–‡ä»¶å¤¹åŒæ­¥åˆ°æ²™ç®±ã€‚

        ç”¨äºåœ¨æ›´æ–°æœ¬åœ° skills/data/memories æ–‡ä»¶å¤¹åï¼Œ
        æ— éœ€é‡å¯åº”ç”¨å³å¯å°†å˜æ›´åŒæ­¥åˆ° Daytona æ²™ç®±ã€‚
        """
        global DAYTONA_SANDBOX

        if DAYTONA_SANDBOX is None:
            raise HTTPException(
                status_code=503, detail="Daytona æ²™ç®±æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆé‡å¯åº”ç”¨"
            )

        try:
            print("\nğŸ”„ æ”¶åˆ°åŒæ­¥è¯·æ±‚ï¼Œå¼€å§‹åŒæ­¥æ–‡ä»¶å¤¹åˆ°æ²™ç®±...")
            sync_folders_to_sandbox(DAYTONA_SANDBOX)
            return {
                "status": "success",
                "message": "æ–‡ä»¶å¤¹åŒæ­¥å®Œæˆ",
                "synced_folders": ["skills", "data", "memories"],
            }
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"åŒæ­¥å¤±è´¥: {str(e)}") from e

    @app.get("/")
    def index() -> HTMLResponse:
        html = (FRONTEND_DIR / "index.html").read_text(encoding="utf-8")
        version = _frontend_asset_version()
        rendered = html.replace("__ASSET_VERSION__", version)
        return HTMLResponse(rendered)

    @app.post("/api/chat")
    def chat(payload: ChatRequest) -> dict[str, Any]:
        if not payload.message.strip():
            raise HTTPException(status_code=400, detail="message cannot be empty")

        try:
            answer, thread_id, history = chat_with_agent(
                message=payload.message,
                history=payload.history,
                thread_id=payload.thread_id,
            )
        except Exception as exc:
            raise HTTPException(status_code=500, detail=f"chat failed: {exc}") from exc

        return {
            "reply": answer,
            "thread_id": thread_id,
            "history": history,
        }

    @app.post("/api/chat/stream")
    async def chat_stream(payload: ChatRequest) -> StreamingResponse:
        if not payload.message.strip():
            raise HTTPException(status_code=400, detail="message cannot be empty")

        async def event_generator() -> AsyncIterator[str]:
            async for event in stream_chat_with_agent(
                message=payload.message,
                history=payload.history,
                thread_id=payload.thread_id,
            ):
                yield _to_sse(event)

        return StreamingResponse(event_generator(), media_type="text/event-stream")

    return app


app = create_app()


def main() -> None:
    auto_reload = os.getenv("AUTO_RELOAD", "0") == "1"
    port = int(os.getenv("PORT", "8005"))

    import signal

    def signal_handler(signum, frame):
        print(f"\næ¥æ”¶åˆ°ä¿¡å· {signum}ï¼Œæ­£åœ¨å…³é—­...")
        cleanup_daytona()
        sys.exit(0)

    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

    try:
        uvicorn.run("main:app", host="0.0.0.0", port=port, reload=auto_reload)
    finally:
        # ç¡®ä¿æ²™ç®±è¢«æ¸…ç†
        cleanup_daytona()


if __name__ == "__main__":
    main()
