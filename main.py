import csv
import json
import os
import re
import sys
import threading
import uuid
from datetime import date, datetime, timedelta
from pathlib import Path
from typing import Any, AsyncIterator

import uvicorn
from deepagents import create_deep_agent
from deepagents.backends import FilesystemBackend
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException
from fastapi.responses import HTMLResponse, StreamingResponse
from fastapi.staticfiles import StaticFiles
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.memory import MemorySaver
from langgraph.store.memory import InMemoryStore
from pydantic import BaseModel, Field

from src.output_sanitize_config import load_output_sanitize_config
from src.prompt_config import PromptConfig

# å°è¯•å¯¼å…¥ Daytonaï¼ˆå¦‚æœæœªå®‰è£…åˆ™ç»™å‡ºå‹å¥½æç¤ºï¼‰
from daytona import CreateSandboxBaseParams, Daytona, FileUpload
from langchain_daytona import DaytonaSandbox

DAYTONA_AVAILABLE = True

# é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).resolve().parent
FRONTEND_DIR = PROJECT_ROOT / "frontend"
SKILLS_DIR = PROJECT_ROOT / "skills"
DATA_DIR = PROJECT_ROOT / "data"
MEMORIES_DIR = PROJECT_ROOT / "memories"
DAILY_DIR = MEMORIES_DIR / "daily"
LONG_TERM_FILE = MEMORIES_DIR / "MEMORY.md"

# Daytona å…¨å±€å®ä¾‹
DAYTONA_INSTANCE: Daytona | None = None
DAYTONA_SANDBOX: Any | None = None

load_dotenv()
from src.output_sanitize_config import load_output_sanitize_config
from src.prompt_config import PromptConfig

# å€’è®¡æ—¶ MCP æœåŠ¡é…ç½®
COUNTDOWN_MCP_URL = os.getenv("COUNTDOWN_MCP_URL", "http://127.0.0.1:8765")

PROMPTS = PromptConfig(PROJECT_ROOT)
OUTPUT_SANITIZE_CONFIG = load_output_sanitize_config(
    PROJECT_ROOT / "config" / "output_sanitize.yaml"
)

model_name = PROMPTS.get_model_name("main")
memory_model_name = PROMPTS.get_model_name("memory_agent")
MEMORY_WRITE_LOCK = threading.Lock()


def _env_flag(name: str, default: bool = True) -> bool:
    raw = os.getenv(name)
    if raw is None:
        return default
    return raw.strip().lower() in {"1", "true", "yes", "on"}


def upload_directory_to_sandbox(
    sandbox, local_dir: Path, remote_base: str, label: str = "æ–‡ä»¶"
):
    """å°†æœ¬åœ°ç›®å½•ä¸Šä¼ åˆ°æ²™ç®±ã€‚"""
    print(f"\nğŸ“¤ æ­£åœ¨ä¸Šä¼  {label} æ–‡ä»¶å¤¹åˆ°æ²™ç®± {remote_base}...")

    upload_files = []

    if not local_dir.exists():
        print(f"âš ï¸  æœ¬åœ° {label} ç›®å½•ä¸å­˜åœ¨: {local_dir}")
        return

    # éå†ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶
    for file_path in local_dir.rglob("*"):
        if file_path.is_file():
            # è®¡ç®—ç›¸å¯¹è·¯å¾„
            rel_path = file_path.relative_to(local_dir)
            remote_path = f"{remote_base}/{rel_path}"

            # è¯»å–æ–‡ä»¶å†…å®¹
            try:
                with open(file_path, "rb") as f:
                    content = f.read()

                upload_files.append(FileUpload(source=content, destination=remote_path))
            except Exception as e:
                print(f"âš ï¸  è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")

    if upload_files:
        # æ‰¹é‡ä¸Šä¼ æ–‡ä»¶
        sandbox.fs.upload_files(upload_files)
        print(f"âœ… å·²ä¸Šä¼  {len(upload_files)} ä¸ª {label} æ–‡ä»¶åˆ°æ²™ç®±")
    else:
        print(f"âš ï¸  æ²¡æœ‰ {label} æ–‡ä»¶éœ€è¦ä¸Šä¼ ")


def upload_skills_to_sandbox(
    sandbox, local_skills_dir: Path, remote_base: str = "/home/daytona/skills"
):
    """å°† skills æ–‡ä»¶å¤¹ä¸Šä¼ åˆ°æ²™ç®±ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰ã€‚"""
    upload_directory_to_sandbox(sandbox, local_skills_dir, remote_base, "skills")


_SANDBOX_ID_FILE = Path(__file__).resolve().parent / ".daytona_sandbox_id"


def _save_sandbox_id(sandbox_id: str):
    """ä¿å­˜æ²™ç®± ID åˆ°æ–‡ä»¶ï¼Œä¾›å…¶ä»–è¿›ç¨‹è¯»å–ã€‚"""
    try:
        _SANDBOX_ID_FILE.write_text(sandbox_id, encoding="utf-8")
    except Exception as e:
        print(f"âš ï¸  ä¿å­˜æ²™ç®± ID å¤±è´¥: {e}")


def _load_sandbox_id() -> str | None:
    """ä»æ–‡ä»¶è¯»å–æ²™ç®± IDã€‚"""
    try:
        if _SANDBOX_ID_FILE.exists():
            return _SANDBOX_ID_FILE.read_text(encoding="utf-8").strip()
    except Exception:
        pass
    return None


def _clear_sandbox_id():
    """æ¸…é™¤æ²™ç®± ID æ–‡ä»¶ã€‚"""
    try:
        if _SANDBOX_ID_FILE.exists():
            _SANDBOX_ID_FILE.unlink()
    except Exception:
        pass


def create_daytona_backend_with_skills(ngrok_url: str | None = None):
    """åˆ›å»º Daytona Sandboxï¼Œä¸Šä¼  skills å’Œ dataï¼Œå¹¶è¿”å› backendã€‚

    å‚æ•°:
        ngrok_url: ngrok URLï¼Œç”¨äºè·å– IP ç™½åå•ã€‚å¦‚æœæä¾›ï¼Œå°†å…è®¸æ²™ç®±è®¿é—®è¯¥ IPã€‚
                 ä¾‹å¦‚: https://nell-pluteal-doria.ngrok-free.dev

    è¿”å›:
        tuple: (backend, daytona, sandbox)
    """
    global DAYTONA_INSTANCE, DAYTONA_SANDBOX

    # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ²™ç®± IDï¼ˆå…¶ä»–è¿›ç¨‹åˆ›å»ºçš„ï¼‰
    existing_sandbox_id = _load_sandbox_id()

    # åˆå§‹åŒ– Daytona
    daytona = Daytona()
    DAYTONA_INSTANCE = daytona

    # å¦‚æœæœ‰ç°æœ‰æ²™ç®± IDï¼Œå°è¯•è¿æ¥
    if existing_sandbox_id:
        try:
            print(f"ğŸ” å°è¯•è¿æ¥ç°æœ‰æ²™ç®±: {existing_sandbox_id}")
            sandbox = daytona.get(existing_sandbox_id)
            # æµ‹è¯•æ²™ç®±æ˜¯å¦å¯ç”¨
            test_result = sandbox.process.exec('echo "ping"')
            if test_result.exit_code == 0:
                print(f"âœ… æˆåŠŸè¿æ¥åˆ°ç°æœ‰æ²™ç®±: {sandbox.id}")
                DAYTONA_SANDBOX = sandbox
                backend = DaytonaSandbox(sandbox=sandbox)
                return backend, daytona, sandbox
            else:
                print("âš ï¸  ç°æœ‰æ²™ç®±ä¸å¯ç”¨ï¼Œåˆ›å»ºæ–°æ²™ç®±")
        except Exception as e:
            print(f"âš ï¸  è¿æ¥ç°æœ‰æ²™ç®±å¤±è´¥: {e}ï¼Œå°†åˆ›å»ºæ–°æ²™ç®±")

    print("ğŸš€ åˆ›å»ºæ–°çš„ Daytona æ²™ç®±...")

    # å‡†å¤‡æ²™ç®±å‚æ•° - è®¾ç½®è‡ªåŠ¨åœæ­¢é—´éš”ä¸º Noneï¼ˆæ°¸ä¸è‡ªåŠ¨åœæ­¢ï¼‰
    params = CreateSandboxBaseParams(
        auto_stop_interval=None,  # None = æ°¸ä¸è‡ªåŠ¨åœæ­¢
    )

    # åˆ›å»ºæ²™ç®±
    sandbox = daytona.create(params)
    DAYTONA_SANDBOX = sandbox

    # ä¿å­˜æ²™ç®± ID
    _save_sandbox_id(sandbox.id)

    print(f"âœ… æ²™ç®±åˆ›å»ºæˆåŠŸ: {sandbox.id}")

    # ä¸Šä¼  skills æ–‡ä»¶å¤¹
    upload_skills_to_sandbox(sandbox, SKILLS_DIR, "/home/daytona/skills")

    # ä¸Šä¼  data æ–‡ä»¶å¤¹åˆ° sales_cli.py æœŸæœ›çš„ä½ç½®
    # sales_cli.py ä½¿ç”¨: Path(__file__).resolve().parent.parent / "data"
    # è„šæœ¬åœ¨: /home/daytona/skills/ä¸‡é”€é”€å”®åœºæ™¯/scripts/
    # æ‰€ä»¥ data åº”è¯¥åœ¨: /home/daytona/skills/ä¸‡é”€é”€å”®åœºæ™¯/data/
    # ä¸Šä¼  data æ–‡ä»¶å¤¹åˆ° sales_cli.py æœŸæœ›çš„ä½ç½®
    # sales_cli.py ä½¿ç”¨: Path(__file__).resolve().parent.parent / "data"
    # è„šæœ¬åœ¨: /home/daytona/skills/ä¸‡é”€é”€å”®åœºæ™¯/scripts/
    # æ‰€ä»¥ data åº”è¯¥åœ¨: /home/daytona/skills/ä¸‡é”€é”€å”®åœºæ™¯/data/
    upload_directory_to_sandbox(
        sandbox,
        DATA_DIR,
        "/home/daytona/data",
        "data",
    )

    # ä¸Šä¼  memories æ–‡ä»¶å¤¹åˆ°æ²™ç®±ï¼ˆä¾› MemoryMiddleware ä½¿ç”¨ï¼‰
    upload_directory_to_sandbox(
        sandbox,
        MEMORIES_DIR,
        "/home/daytona/memories",
        "memories",
    )

    # éªŒè¯ä¸Šä¼ 
    print("\nğŸ” éªŒè¯ä¸Šä¼ çš„æ–‡ä»¶...")
    ls_result = sandbox.process.exec("find /home/daytona/skills -type f | head -10")
    print(f"æ²™ç®±ä¸­çš„ skills æ–‡ä»¶:\n{ls_result.result}")

    # éªŒè¯ data æ–‡ä»¶å¤¹å­˜åœ¨ä¸”åŒ…å« customer_db.csv
    ls_data_result = sandbox.process.exec("ls -la /home/daytona/data")
    print(f"\næ²™ç®±ä¸­çš„ data æ–‡ä»¶:\n{ls_data_result.result}")

    # æ£€æŸ¥ customer_db.csv æ˜¯å¦å­˜åœ¨
    check_csv = sandbox.process.exec(
        "test -f /home/daytona/data/customer_db.csv && echo 'âœ… customer_db.csv å­˜åœ¨' || echo 'âŒ customer_db.csv ä¸å­˜åœ¨'"
    )
    print(f"\n{check_csv.result}")

    # éªŒè¯ memories æ–‡ä»¶å¤¹å­˜åœ¨
    ls_memories_result = sandbox.process.exec("ls -la /home/daytona/memories")
    print(f"\næ²™ç®±ä¸­çš„ memories æ–‡ä»¶:\n{ls_memories_result.result}")

    # æ£€æŸ¥ MEMORY.md æ˜¯å¦å­˜åœ¨
    check_memory = sandbox.process.exec(
        "test -f /home/daytona/memories/MEMORY.md && echo 'âœ… MEMORY.md å­˜åœ¨' || echo 'âŒ MEMORY.md ä¸å­˜åœ¨'"
    )
    print(f"\n{check_memory.result}")

    # ä½¿ç”¨ DaytonaSandbox ä½œä¸º backend
    backend = DaytonaSandbox(sandbox=sandbox)
    print("âœ… DaytonaSandbox backend åˆ›å»ºæˆåŠŸ")

    return backend, daytona, sandbox


def cleanup_daytona():
    """æ¸…ç† Daytona æ²™ç®±ã€‚"""
    global DAYTONA_INSTANCE, DAYTONA_SANDBOX
    if DAYTONA_INSTANCE and DAYTONA_SANDBOX:
        print("\nğŸ§¹ æ¸…ç† Daytona æ²™ç®±...")
        try:
            DAYTONA_INSTANCE.delete(DAYTONA_SANDBOX)
            print("âœ… æ²™ç®±å·²åˆ é™¤")
        except Exception as e:
            print(f"âš ï¸  åˆ é™¤æ²™ç®±æ—¶å‡ºé”™: {e}")
    # æ¸…é™¤æ²™ç®± ID æ–‡ä»¶
    _clear_sandbox_id()


MEMORY_AGENT_ENABLED = _env_flag("MEMORY_AGENT_ENABLED", default=True)
OUTPUT_SANITIZE_ENABLED = _env_flag("OUTPUT_SANITIZE_ENABLED", default=True)
CHECKPOINTER = MemorySaver()


def _ensure_memory_files(today: date) -> tuple[str, str]:
    MEMORIES_DIR.mkdir(parents=True, exist_ok=True)
    DAILY_DIR.mkdir(parents=True, exist_ok=True)

    today_name = today.strftime("%Y-%m-%d")
    today_file = DAILY_DIR / f"{today_name}.md"

    if not LONG_TERM_FILE.exists():
        LONG_TERM_FILE.write_text(
            "# é•¿æœŸè®°å¿†\n\n"
            "## ç”¨æˆ·åå¥½\n"
            "- æš‚æ— \n\n"
            "## é‡è¦å†³ç­–\n"
            "- æš‚æ— \n\n"
            "## å…³é”®è”ç³»äºº\n"
            "- æš‚æ— \n\n"
            "## é¡¹ç›®äº‹å®\n"
            "- æš‚æ— \n",
            encoding="utf-8",
        )

    if not today_file.exists():
        today_file.write_text(
            f"# {today_name}\n\n"
            "## 09:00 - ä¼šè¯åˆå§‹åŒ–\n"
            "- æ–°çš„ä¸€å¤©å¼€å§‹ï¼ŒæŒ‰éœ€è®°å½•é‡è¦äº‹å®ã€å†³ç­–ã€åå¥½ä¸å¾…åŠã€‚\n",
            encoding="utf-8",
        )

    return (
        "/memories/MEMORY.md",
        f"/memories/daily/{today_name}.md",
    )


def build_agent() -> Any:
    # api_key = os.getenv("OPENROUTER_API_KEY")
    api_key = os.getenv("GLM_API_KEY")
    # if not api_key:
    #     raise RuntimeError("OPENROUTER_API_KEY is missing. Please set it in .env")
    #
    if not api_key:
        raise RuntimeError("GLM_API_KEY is missing. Please set it in .env")

    os.environ["OPENAI_API_KEY"] = api_key
    # llm = ChatOpenAI(
    #     model=model_name,
    #     base_url="https://openrouter.ai/api/v1",
    #     # temperature=0.2,
    # )
    llm = ChatOpenAI(
        model=model_name,
        base_url="https://open.bigmodel.cn/api/paas/v4",
        temperature=0.1,
    )

    today = date.today()
    yesterday = today - timedelta(days=1)
    long_term_path, today_path = _ensure_memory_files(today)
    yesterday_path = f"/memories/daily/{yesterday.strftime('%Y-%m-%d')}.md"

    system_prompt = PROMPTS.render_prompt(
        "main_system",
        {
            "long_term_path": str(long_term_path),
            "today_path": str(today_path),
            "yesterday_path": str(yesterday_path),
        },
    )

    # ä½¿ç”¨ Daytona Sandbox ä½œä¸º backendï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if DAYTONA_AVAILABLE:
        print("ğŸš€ ä½¿ç”¨ Daytona Sandbox backend")
        backend, _, _ = create_daytona_backend_with_skills()
        # skills ä½¿ç”¨æ²™ç®±ä¸­çš„è·¯å¾„
        skills = ["/home/daytona/skills"]
        # è®°å¿†æ–‡ä»¶ä½¿ç”¨æ²™ç®±ä¸­çš„è·¯å¾„
        memory_paths = [
            "/home/daytona/memories/MEMORY.md",
            f"/home/daytona/memories/daily/{today.strftime('%Y-%m-%d')}.md",
            f"/home/daytona/memories/daily/{yesterday.strftime('%Y-%m-%d')}.md",
        ]
    else:
        print("âš ï¸  Daytona ä¸å¯ç”¨ï¼Œä½¿ç”¨æœ¬åœ° FilesystemBackend")
        from deepagents.backends import FilesystemBackend

        backend = FilesystemBackend(root_dir=str(PROJECT_ROOT))
        skills = [str(SKILLS_DIR)]
        # è®°å¿†æ–‡ä»¶ä½¿ç”¨æœ¬åœ°è™šæ‹Ÿè·¯å¾„
        memory_paths = [long_term_path, today_path, yesterday_path]
    agent = create_deep_agent(
        model=llm,
        store=InMemoryStore(),
        backend=backend,
        skills=skills,
        memory=memory_paths,
        checkpointer=CHECKPOINTER,
        system_prompt=system_prompt,
    )

    return agent


def build_memory_agent(backend: Any | None = None) -> Any:
    """æ„å»º Memory Agentã€‚
    
    Args:
        backend: å¯é€‰çš„ backend å®ä¾‹ã€‚å¦‚æœæä¾›ï¼Œå°†ä½¿ç”¨è¯¥ backend è€Œä¸æ˜¯åˆ›å»ºæ–°çš„ã€‚
                 è¿™æ ·å¯ä»¥ç¡®ä¿ Memory Agent å’Œä¸» Agent è®¿é—®ç›¸åŒçš„æ–‡ä»¶ç³»ç»Ÿã€‚
    """
    api_key = os.getenv("OPENROUTER_API_KEY")
    if not api_key:
        raise RuntimeError("OPENROUTER_API_KEY is missing. Please set it in .env")

    os.environ["OPENAI_API_KEY"] = api_key
    llm = ChatOpenAI(
        model=memory_model_name,
        base_url="https://openrouter.ai/api/v1",
        temperature=0.1,
    )

    today = date.today()
    yesterday = today - timedelta(days=1)
    long_term_path, today_path = _ensure_memory_files(today)
    yesterday_path = f"/memories/daily/{yesterday.strftime('%Y-%m-%d')}.md"
    
    # æ„å»ºè®°å¿†è·¯å¾„åˆ—è¡¨ï¼ˆä¸ä¸» Agent ç›¸åŒï¼‰
    if DAYTONA_AVAILABLE and backend is not None:
        # ä½¿ç”¨ä¸ä¸» Agent ç›¸åŒçš„ Daytona backend
        memory_paths = [
            "/home/daytona/memories/MEMORY.md",
            f"/home/daytona/memories/daily/{today.strftime('%Y-%m-%d')}.md",
            f"/home/daytona/memories/daily/{yesterday.strftime('%Y-%m-%d')}.md",
        ]
    else:
        # ä½¿ç”¨æœ¬åœ° FilesystemBackend
        memory_paths = [long_term_path, today_path, yesterday_path]
    
    # å¦‚æœæ²¡æœ‰æä¾› backendï¼Œåˆ›å»ºé»˜è®¤çš„æœ¬åœ° backend
    if backend is None:
        backend = FilesystemBackend(root_dir=str(PROJECT_ROOT), virtual_mode=True)

    system_prompt = PROMPTS.render_prompt("memory_agent")
    agent = create_deep_agent(
        model=llm,
        backend=backend,
        memory=memory_paths,
        system_prompt=system_prompt,
    )
    return agent


def _list_skill_packages(skills_dir: Path) -> list[dict[str, str]]:
    if not skills_dir.exists():
        return []

    seen: set[str] = set()
    packages: list[dict[str, str]] = []
    for skill_file in skills_dir.rglob("SKILL.md"):
        parent_dir = skill_file.parent
        try:
            rel = parent_dir.relative_to(skills_dir).as_posix()
        except ValueError:
            continue

        if rel in seen:
            continue

        seen.add(rel)
        packages.append({"name": parent_dir.name, "path": f"/skills/{rel}"})

    packages.sort(key=lambda item: (item["name"], item["path"]))
    return packages


def _customers_payload(csv_path: Path) -> list[dict[str, str]]:
    if not csv_path.exists() or not csv_path.is_file():
        return []

    customers: list[dict[str, str]] = []
    with csv_path.open("r", encoding="utf-8", newline="") as handle:
        reader = csv.DictReader(handle)
        for row in reader:
            if not isinstance(row, dict):
                continue

            name = str(row.get("name", "")).strip()
            age = str(row.get("age", "")).strip()
            gender = str(row.get("gender", "")).strip()
            behavior = str(row.get("behaviors", "")).strip()
            if not name:
                continue

            customers.append(
                {
                    "name": name,
                    "age": age,
                    "gender": gender,
                    "behavior": behavior,
                }
            )

    return customers


def _frontend_asset_version() -> str:
    candidates = [
        FRONTEND_DIR / "index.html",
        FRONTEND_DIR / "style.css",
        FRONTEND_DIR / "script.js",
    ]
    latest_mtime = 0.0
    for path in candidates:
        if path.exists():
            latest_mtime = max(latest_mtime, path.stat().st_mtime)

    return str(int(latest_mtime)) if latest_mtime else "1"


def _latest_daily_memory_files(limit: int = 2) -> list[Path]:
    if not DAILY_DIR.exists():
        return []

    files = [p for p in DAILY_DIR.glob("*.md") if p.is_file()]
    files.sort(key=lambda p: p.name, reverse=True)
    return files[:limit]


def _read_text_file(path: Path) -> str:
    if not path.exists() or not path.is_file():
        return ""
    return path.read_text(encoding="utf-8", errors="replace")


def _memories_payload() -> dict[str, Any]:
    today = date.today()
    _ensure_memory_files(today)

    short_files = _latest_daily_memory_files(limit=2)
    short_term: list[dict[str, str]] = []
    max_mtime = LONG_TERM_FILE.stat().st_mtime if LONG_TERM_FILE.exists() else 0.0

    for file_path in short_files:
        short_term.append(
            {
                "date": file_path.stem,
                "path": f"/memories/daily/{file_path.name}",
                "content": _read_text_file(file_path),
            }
        )
        max_mtime = max(max_mtime, file_path.stat().st_mtime)

    long_term_content = _read_text_file(LONG_TERM_FILE)
    if LONG_TERM_FILE.exists():
        max_mtime = max(max_mtime, LONG_TERM_FILE.stat().st_mtime)

    return {
        "version": str(int(max_mtime)) if max_mtime else "1",
        "short_term": short_term,
        "long_term": {
            "path": "/memories/MEMORY.md",
            "content": long_term_content,
        },
    }


# é¢„åŠ è½½çš„ Agent å®ä¾‹ï¼ˆåº”ç”¨å¯åŠ¨æ—¶åˆå§‹åŒ–ï¼‰
AGENT: Any | None = None
MEMORY_AGENT: Any | None = None


_AGENTS_INITIALIZED = False
_INIT_LOCK_FILE = Path(__file__).resolve().parent / ".agents_init.lock"


def _is_another_process_initializing() -> bool:
    """æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–è¿›ç¨‹æ­£åœ¨åˆå§‹åŒ–ã€‚"""
    try:
        # å°è¯•åˆ›å»ºé”æ–‡ä»¶
        fd = os.open(str(_INIT_LOCK_FILE), os.O_CREAT | os.O_EXCL | os.O_WRONLY)
        os.close(fd)
        return False
    except FileExistsError:
        # é”æ–‡ä»¶å·²å­˜åœ¨ï¼Œè¯´æ˜å…¶ä»–è¿›ç¨‹æ­£åœ¨åˆå§‹åŒ–
        return True


def _wait_for_initialization(timeout: float = 30.0) -> bool:
    """ç­‰å¾…å…¶ä»–è¿›ç¨‹å®Œæˆåˆå§‹åŒ–ã€‚"""
    import time

    start = time.time()
    while time.time() - start < timeout:
        if not _INIT_LOCK_FILE.exists():
            # é”æ–‡ä»¶è¢«åˆ é™¤ï¼Œè¯´æ˜åˆå§‹åŒ–å®Œæˆ
            return True
        time.sleep(0.5)
    return False


def _cleanup_lock_file():
    """æ¸…ç†é”æ–‡ä»¶ã€‚"""
    try:
        if _INIT_LOCK_FILE.exists():
            _INIT_LOCK_FILE.unlink()
    except Exception:
        pass


def init_agents() -> None:
    """åœ¨åº”ç”¨å¯åŠ¨æ—¶é¢„åŠ è½½æ‰€æœ‰ Agent å’Œæ²™ç®±ã€‚"""
    global AGENT, MEMORY_AGENT, _AGENTS_INITIALIZED

    # å¿«é€Ÿæ£€æŸ¥ï¼šå¦‚æœå·²åˆå§‹åŒ–ï¼Œç›´æ¥è¿”å›
    if _AGENTS_INITIALIZED and AGENT is not None:
        print("âš ï¸  Agents å·²åˆå§‹åŒ–ï¼Œè·³è¿‡é‡å¤åˆå§‹åŒ–")
        return

    # æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–è¿›ç¨‹æ­£åœ¨åˆå§‹åŒ–
    if _is_another_process_initializing():
        print("â³ æ£€æµ‹åˆ°å…¶ä»–è¿›ç¨‹æ­£åœ¨åˆå§‹åŒ– Agentsï¼Œç­‰å¾…ä¸­...")
        if _wait_for_initialization():
            print("âœ… å…¶ä»–è¿›ç¨‹åˆå§‹åŒ–å®Œæˆï¼Œå¤ç”¨å·²åˆ›å»ºçš„ Agents")
            # æ³¨æ„ï¼šè¿™é‡Œ AGENT å¯èƒ½è¿˜æ˜¯ Noneï¼ˆworker è¿›ç¨‹æ— æ³•è®¿é—®ä¸»è¿›ç¨‹çš„å†…å­˜ï¼‰
            # ä½†æ²™ç®±å·²ç»ç”±ä¸»è¿›ç¨‹åˆ›å»ºå¥½äº†
            if AGENT is None:
                # Worker è¿›ç¨‹éœ€è¦é‡æ–°åˆå§‹åŒ–ï¼ˆä½†æ²™ç®±å·²å­˜åœ¨ï¼Œä¸ä¼šé‡å¤åˆ›å»ºï¼‰
                pass
        return

    try:
        print("=" * 60)
        print("ğŸš€ é¢„åŠ è½½ Agents...")
        print("=" * 60)

        # é¢„åŠ è½½ä¸» Agentï¼ˆä¼šåˆ›å»º Daytona æ²™ç®±ï¼‰
        print("\n[1/2] åˆå§‹åŒ–ä¸» Agent (Daytona Sandbox)...")
        AGENT = build_agent()
        print("âœ… ä¸» Agent åˆå§‹åŒ–å®Œæˆ")

        # é¢„åŠ è½½ Memory Agentï¼ˆä½¿ç”¨ä¸ä¸» Agent ç›¸åŒçš„ backendï¼‰
        if MEMORY_AGENT_ENABLED:
            print("\n[2/2] åˆå§‹åŒ– Memory Agent...")
            # è·å–ä¸» Agent çš„ backend
            main_agent_backend = None
            if hasattr(AGENT, '_backend'):
                main_agent_backend = AGENT._backend
            elif hasattr(AGENT, 'backend'):
                main_agent_backend = AGENT.backend
            
            MEMORY_AGENT = build_memory_agent(backend=main_agent_backend)
            print("âœ… Memory Agent åˆå§‹åŒ–å®Œæˆ")
        else:
            print("\n[2/2] Memory Agent å·²ç¦ç”¨ï¼Œè·³è¿‡åˆå§‹åŒ–")

        _AGENTS_INITIALIZED = True
        print("\n" + "=" * 60)
        print("âœ… æ‰€æœ‰ Agents é¢„åŠ è½½å®Œæˆï¼")
        print("=" * 60 + "\n")

    except Exception as e:
        print(f"âŒ åˆå§‹åŒ– Agents å¤±è´¥: {e}")
        raise
    finally:
        # å»¶è¿Ÿæ¸…ç†é”æ–‡ä»¶ï¼Œç¡®ä¿å…¶ä»–è¿›ç¨‹æœ‰è¶³å¤Ÿæ—¶é—´æ£€æµ‹åˆ°åˆå§‹åŒ–å®Œæˆ
        import time

        time.sleep(1)
        _cleanup_lock_file()

def get_agent() -> Any:
    """è·å–å·²é¢„åŠ è½½çš„ä¸» Agentã€‚"""
    if AGENT is None:
        raise RuntimeError("Agent å°šæœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨ init_agents()")
    return AGENT


def get_memory_agent() -> Any:
    """è·å–å·²é¢„åŠ è½½çš„ Memory Agentã€‚"""
    if MEMORY_AGENT is None:
        raise RuntimeError("Memory Agent å°šæœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨ init_agents()")
    return MEMORY_AGENT


def _now_hhmm() -> str:
    return datetime.now().strftime("%H:%M")


def _append_daily_memory(title: str, bullets: list[str], today_path: Path) -> None:
    cleaned_title = title.strip() or "ä¼šè¯è®°å½•"
    cleaned_bullets = [x.strip() for x in bullets if x and x.strip()]
    if not cleaned_bullets:
        return

    block_lines = [f"## {_now_hhmm()} - {cleaned_title}"]
    for line in cleaned_bullets:
        block_lines.append(f"- {line}")
    block = "\n" + "\n".join(block_lines) + "\n"

    current = _read_text_file(today_path)
    today_path.write_text(current + block, encoding="utf-8")


def _parse_long_term_sections(content: str) -> dict[str, list[str]]:
    section_names = ["ç”¨æˆ·åå¥½", "é‡è¦å†³ç­–", "å…³é”®è”ç³»äºº", "é¡¹ç›®äº‹å®"]
    sections: dict[str, list[str]] = {name: [] for name in section_names}
    current_section = ""

    for raw in content.splitlines():
        line = raw.strip()
        if line.startswith("## "):
            name = line[3:].strip()
            current_section = name if name in sections else ""
            continue
        if current_section and line.startswith("-"):
            item = line[1:].strip()
            if item and item != "æš‚æ— ":
                sections[current_section].append(item)

    return sections


def _render_long_term_sections(sections: dict[str, list[str]]) -> str:
    ordered_names = ["ç”¨æˆ·åå¥½", "é‡è¦å†³ç­–", "å…³é”®è”ç³»äºº", "é¡¹ç›®äº‹å®"]
    lines = ["# é•¿æœŸè®°å¿†", ""]
    for name in ordered_names:
        lines.append(f"## {name}")
        items = []
        seen: set[str] = set()
        for text in sections.get(name, []):
            normalized = text.strip()
            if not normalized or normalized in seen:
                continue
            seen.add(normalized)
            items.append(normalized)
        if not items:
            lines.append("- æš‚æ— ")
        else:
            for item in items:
                lines.append(f"- {item}")
        lines.append("")
    return "\n".join(lines).rstrip() + "\n"


def _merge_long_term_memory(
    updates: dict[str, list[str]], long_term_path: Path
) -> None:
    current = _read_text_file(long_term_path)
    sections = _parse_long_term_sections(current)

    for key, values in updates.items():
        if key not in sections or not isinstance(values, list):
            continue
        for value in values:
            if isinstance(value, str) and value.strip():
                sections[key].append(value.strip())

    long_term_path.write_text(_render_long_term_sections(sections), encoding="utf-8")


def _safe_json_loads(text: str) -> dict[str, Any]:
    try:
        payload = json.loads(text)
        return payload if isinstance(payload, dict) else {}
    except Exception:
        return {}


def _strip_markdown_syntax(text: str) -> str:
    """å°† Markdown æ ¼å¼è½¬æ¢ä¸ºçº¯æ–‡æœ¬ã€‚"""
    if not text:
        return ""

    value = text
    # ä»£ç å— ```
    value = re.sub(r"```[\w-]*\n?", "", value)
    value = value.replace("```", "")
    # è¡Œå†…ä»£ç  `code`
    value = re.sub(r"`([^`]+)`", r"\1", value)
    # ç²—ä½“ **text**
    value = re.sub(r"\*\*([^*]+)\*\*", r"\1", value)
    # æ–œä½“ *text* æˆ– _text_
    value = re.sub(r"\*([^*]+)\*", r"\1", value)
    value = re.sub(r"_([^_]+)_", r"\1", value)
    # é«˜äº® ==text==
    value = re.sub(r"==([^=]+)==", r"\1", value)
    # åˆ é™¤çº¿ ~~text~~
    value = re.sub(r"~~([^~]+)~~", r"\1", value)
    # é“¾æ¥ [text](url) -> åªä¿ç•™ text
    value = re.sub(r"\[([^\]]+)\]\([^)]+\)", r"\1", value)
    # å›¾ç‰‡ ![alt](url) -> ç§»é™¤
    value = re.sub(r"!\[[^\]]*\]\([^)]+\)", "", value)
    # æ ‡é¢˜ # ## ### -> ç§»é™¤ # ç¬¦å·
    value = re.sub(r"^#{1,6}\s*", "", value, flags=re.MULTILINE)
    # å¼•ç”¨ > -> ç§»é™¤ > ç¬¦å·
    value = re.sub(r"^>\s*", "", value, flags=re.MULTILINE)
    # åˆ—è¡¨ - * + 1. -> ç§»é™¤åˆ—è¡¨æ ‡è®°
    value = re.sub(r"^[-*+]\s+", "", value, flags=re.MULTILINE)
    value = re.sub(r"^\d+\.\s+", "", value, flags=re.MULTILINE)
    # æ°´å¹³çº¿ --- *** ___ -> ç§»é™¤
    value = re.sub(r"^[-*_]{3,}\s*$", "", value, flags=re.MULTILINE)
    # HTML æ ‡ç­¾
    value = re.sub(r"<[^>]+>", "", value)
    return value
    if not text:
        return ""

    value = text
    value = re.sub(r"```[\w-]*\n?", "", value)
    value = value.replace("```", "")
    value = re.sub(r"`([^`]+)`", r"\1", value)
    value = re.sub(r"\*\*([^*]+)\*\*", r"\1", value)
    value = re.sub(r"==([^=]+)==", r"\1", value)
    return value


def _is_internal_leak_line(line: str) -> bool:
    if not OUTPUT_SANITIZE_ENABLED or not OUTPUT_SANITIZE_CONFIG.enabled:
        return False

    s = line.strip()
    if not s:
        return False

    if any(token in s for token in OUTPUT_SANITIZE_CONFIG.literals):
        return True

    return any(pattern.search(s) for pattern in OUTPUT_SANITIZE_CONFIG.regex_patterns)


def _sanitize_user_facing_text(text: str) -> str:
    raw = _strip_markdown_syntax(text)
    lines = raw.splitlines()
    kept = [line for line in lines if not _is_internal_leak_line(line)]
    cleaned = "\n".join(kept).strip()
    return cleaned


def _consume_stream_buffer(buffer: str) -> tuple[str, str]:
    if "\n" not in buffer:
        return "", buffer

    parts = buffer.split("\n")
    remain = parts.pop() if parts else ""
    output_parts: list[str] = []
    for raw_line in parts:
        clean_line = _strip_markdown_syntax(raw_line)
        if _is_internal_leak_line(clean_line):
            continue
        output_parts.append(clean_line)

    flushed = "\n".join(output_parts)
    if flushed:
        flushed += "\n"
    return flushed, remain


def _run_memory_agent_sync(user_message: str, assistant_message: str) -> None:
    if not user_message.strip() or not assistant_message.strip():
        return

    with MEMORY_WRITE_LOCK:
        today = date.today()
        _, today_virtual_path = _ensure_memory_files(today)
        today_path = PROJECT_ROOT / today_virtual_path.lstrip("/")
        yesterday = today - timedelta(days=1)
        yesterday_virtual_path = f"/memories/daily/{yesterday.strftime('%Y-%m-%d')}.md"
        yesterday_path = PROJECT_ROOT / yesterday_virtual_path.lstrip("/")

        long_term_path = LONG_TERM_FILE
        today_log_content = _read_text_file(today_path)
        yesterday_log_content = _read_text_file(yesterday_path)
        long_term_content = _read_text_file(long_term_path)

        memory_task = (
            "è¯·åŸºäºä»¥ä¸‹æœ¬è½®ä¼šè¯æ›´æ–°è®°å¿†æ–‡ä»¶ï¼Œå¹¶ç›´æ¥ä½¿ç”¨æ–‡ä»¶å·¥å…·è½ç›˜ã€‚\n\n"
            f"ç”¨æˆ·æ¶ˆæ¯:\n{user_message}\n\n"
            f"åŠ©æ‰‹å›å¤:\n{assistant_message}\n\n"
            f"ä»Šæ—¥æ—¥å¿—è·¯å¾„: {today_virtual_path}\n"
            f"æ˜¨æ—¥æ—¥å¿—è·¯å¾„: {yesterday_virtual_path}\n"
            "é•¿æœŸè®°å¿†è·¯å¾„: /memories/MEMORY.md\n\n"
            "ä»Šå¤©æ—¥å¿—å½“å‰å†…å®¹:\n"
            f"{today_log_content or '(ç©º)'}\n\n"
            "æ˜¨å¤©æ—¥å¿—å½“å‰å†…å®¹:\n"
            f"{yesterday_log_content or '(ç©º)'}\n\n"
            "é•¿æœŸè®°å¿†å½“å‰å†…å®¹:\n"
            f"{long_term_content or '(ç©º)'}\n"
        )

        get_memory_agent().invoke(
            {"messages": [{"role": "user", "content": memory_task}]},
            config={
                "configurable": {"thread_id": f"memory-{today.strftime('%Y-%m-%d')}"}
            },
        )


def _dispatch_memory_agent(user_message: str, assistant_message: str) -> None:
    if not MEMORY_AGENT_ENABLED:
        return

    worker = threading.Thread(
        target=_run_memory_agent_sync,
        args=(user_message, assistant_message),
        daemon=True,
    )
    worker.start()


def _truncate_history(history: list[Any], max_rounds: int = 3) -> list[Any]:
    """æˆªæ–­å†å²å¯¹è¯ï¼Œåªä¿ç•™æœ€è¿‘ N è½® + å½“å‰è½®ã€‚

    Args:
        history: å®Œæ•´çš„å†å²å¯¹è¯åˆ—è¡¨
        max_rounds: ä¿ç•™çš„å¯¹è¯è½®æ•°ï¼ˆé»˜è®¤ 3 è½®ï¼‰

    Returns:
        æˆªæ–­åçš„å†å²åˆ—è¡¨
    """
    if not history:
        return []

    # è¿‡æ»¤å‡ºæœ‰æ•ˆçš„å¯¹è¯è½®ï¼ˆuser + assistant ä¸ºä¸€å¯¹ï¼‰
    valid_messages = []
    for item in history:
        if isinstance(item, dict):
            role = item.get('role')
            if role in {'user', 'assistant'}:
                valid_messages.append(item)
        elif isinstance(item, (list, tuple)) and len(item) == 2:
            # å¤„ç† [user_msg, assistant_msg] æ ¼å¼
            user_msg, assistant_msg = item
            if user_msg:
                valid_messages.append({'role': 'user', 'content': str(user_msg)})
            if assistant_msg:
                valid_messages.append({'role': 'assistant', 'content': str(assistant_msg)})

    # åªä¿ç•™æœ€è¿‘ N è½®ï¼ˆæ¯è½®åŒ…å« user + assistant ä¸¤æ¡æ¶ˆæ¯ï¼‰
    max_messages = max_rounds * 2
    truncated = valid_messages[-max_messages:] if len(valid_messages) > max_messages else valid_messages

    if len(valid_messages) > max_messages:
        print(f"ğŸ“‰ å†å²å¯¹è¯å·²æˆªæ–­: {len(valid_messages)} æ¡ -> {len(truncated)} æ¡ (ä¿ç•™æœ€è¿‘ {max_rounds} è½®)")

    return truncated


def _to_deepagent_messages(history: list[Any], user_text: str) -> list[dict[str, str]]:
    """å°†å†å²å¯¹è¯è½¬æ¢ä¸º deepagent æ¶ˆæ¯æ ¼å¼ã€‚

    æ³¨æ„ï¼šè¿™é‡Œçš„å†å²ä¼šè¢«æˆªæ–­ï¼Œåªä¿ç•™æœ€è¿‘ 3 è½®ã€‚
    æ—©æœŸå¯¹è¯å†…å®¹å·²é€šè¿‡è®°å¿†ç³»ç»Ÿä¿å­˜åˆ° system promptã€‚
    """
    messages: list[dict[str, str]] = []

    # æˆªæ–­å†å²ï¼Œåªä¿ç•™æœ€è¿‘ 3 è½®
    truncated_history = _truncate_history(history, max_rounds=3)

    for item in truncated_history:
        if isinstance(item, dict):
            role = item.get('role')
            content = item.get('content', '')
            if role in {'user', 'assistant'}:
                messages.append({'role': role, 'content': str(content)})
            continue

        if isinstance(item, (list, tuple)) and len(item) == 2:
            user_msg, assistant_msg = item
            if user_msg:
                messages.append({'role': 'user', 'content': str(user_msg)})
            if assistant_msg:
                messages.append({'role': 'assistant', 'content': str(assistant_msg)})

    messages.append({'role': 'user', 'content': user_text})
    return messages


def chat_with_agent(
    message: str, history: list[Any], thread_id: str | None
) -> tuple[str, str, list[dict[str, str]]]:
    resolved_thread_id = thread_id or str(uuid.uuid4())
    input_messages = [{"role": "user", "content": message}]

    result = get_agent().invoke(
        {"messages": input_messages},
        config={"configurable": {"thread_id": resolved_thread_id}},
    )

    assistant_text = ""
    for msg in reversed(result.get("messages", [])):
        if _get_msg_type(msg) == "ai":
            assistant_text = _message_content_to_text(_get_msg_content(msg))
            break

    assistant_text = _sanitize_user_facing_text(assistant_text)

    updated_history = _to_deepagent_messages(history, message)
    updated_history.append({"role": "assistant", "content": assistant_text})
    _dispatch_memory_agent(user_message=message, assistant_message=assistant_text)
    return assistant_text, resolved_thread_id, updated_history


def _message_content_to_text(content: Any) -> str:
    if isinstance(content, str):
        return content

    if isinstance(content, list):
        parts: list[str] = []
        for item in content:
            if isinstance(item, dict):
                text = item.get("text") or item.get("content")
                if isinstance(text, str):
                    parts.append(text)
            elif hasattr(item, "text"):
                text = getattr(item, "text", "")
                if isinstance(text, str):
                    parts.append(text)
            elif hasattr(item, "content"):
                text = getattr(item, "content", "")
                if isinstance(text, str):
                    parts.append(text)
            elif isinstance(item, str):
                parts.append(item)
        return "".join(parts)

    return ""


def _get_msg_type(msg: Any) -> str:
    if isinstance(msg, dict):
        return str(msg.get("type", ""))
    return str(getattr(msg, "type", ""))


def _get_msg_content(msg: Any) -> Any:
    if isinstance(msg, dict):
        return msg.get("content", "")
    return getattr(msg, "content", "")


def _extract_ai_text_from_output(output: Any) -> str:
    if not isinstance(output, dict):
        return ""

    messages = output.get("messages")
    if not isinstance(messages, list):
        return ""

    for msg in reversed(messages):
        if _get_msg_type(msg) == "ai":
            text = _message_content_to_text(_get_msg_content(msg))
            if text:
                return text
    return ""


def _extract_event_chunk_text(chunk: Any) -> str:
    if chunk is None:
        return ""

    if hasattr(chunk, "content"):
        return _message_content_to_text(getattr(chunk, "content"))

    if isinstance(chunk, dict) and "content" in chunk:
        return _message_content_to_text(chunk.get("content"))

    return _message_content_to_text(chunk)


def _preview_output(value: Any, limit: int = 120) -> str:
    text = _message_content_to_text(value)
    if not text:
        text = str(value)
    text = text.replace("\n", " ").strip()
    if len(text) > limit:
        return f"{text[:limit]}..."
    return text


async def stream_chat_with_agent(
    message: str, history: list[Any], thread_id: str | None
) -> AsyncIterator[dict[str, Any]]:
    resolved_thread_id = thread_id or str(uuid.uuid4())
    input_messages = [{"role": "user", "content": message}]

    assistant_full_text = ""
    started_nodes: set[str] = set()
    completed_nodes: set[str] = set()
    fallback_ai_text = ""
    stream_buffer = ""

    yield {"event": "start", "thread_id": resolved_thread_id}

    try:
        event_iter = get_agent().astream_events(
            {"messages": input_messages},
            config={"configurable": {"thread_id": resolved_thread_id}},
            version="v2",
        )

        async for event in event_iter:
            kind = str(event.get("event", ""))
            name = str(event.get("name", ""))
            data = event.get("data") if isinstance(event.get("data"), dict) else {}
            metadata = (
                event.get("metadata") if isinstance(event.get("metadata"), dict) else {}
            )
            node_name = str(metadata.get("langgraph_node") or name or "")

            if kind == "on_chat_model_stream":
                chunk = data.get("chunk")
                delta = _extract_event_chunk_text(chunk)
                if delta:
                    stream_buffer += delta
                    flushed, stream_buffer = _consume_stream_buffer(stream_buffer)
                    if flushed:
                        assistant_full_text += flushed
                        yield {"event": "delta", "text": flushed}
                continue

            if kind == "on_chain_start":
                if (
                    node_name
                    and node_name not in started_nodes
                    and node_name != "agent"
                ):
                    started_nodes.add(node_name)
                    yield {"event": "process", "text": f"è¿›å…¥é˜¶æ®µ: {node_name}"}
                continue

            if kind == "on_tool_start":
                tool_input = data.get("input")
                yield {"event": "process", "text": f"è°ƒç”¨å·¥å…·: {name}"}
                if tool_input is not None:
                    yield {
                        "event": "process",
                        "text": f"å·¥å…·å‚æ•°: {_preview_output(tool_input)}",
                    }
                continue

            if kind == "on_tool_end":
                tool_output = data.get("output")
                tool_result_preview = ""
                if tool_output is not None:
                    tool_result_preview = _preview_output(tool_output)

                # æ£€æµ‹ start_countdown_async å·¥å…·å®Œæˆï¼Œå‘é€ countdown_started äº‹ä»¶
                if name == "start_countdown_async" and tool_output:
                    task_id = tool_output.get("task_id")
                    countdown_seconds = tool_output.get("countdown_seconds", 10)
                    if task_id:
                        yield {
                            "event": "countdown_started",
                            "task_id": task_id,
                            "countdown_seconds": countdown_seconds,
                        }

                yield {"event": "process", "text": f"å·¥å…·å®Œæˆ: {name}"}
                if tool_result_preview:
                    yield {
                        "event": "process",
                        "text": f"å·¥å…·ç»“æœ: {tool_result_preview}",
                    }
                continue

            if kind == "on_chain_end":
                if (
                    node_name
                    and node_name not in completed_nodes
                    and node_name != "agent"
                ):
                    completed_nodes.add(node_name)
                    yield {"event": "process", "text": f"é˜¶æ®µå®Œæˆ: {node_name}"}

                output = data.get("output")
                if not assistant_full_text and output is not None:
                    fallback = _extract_ai_text_from_output(output)
                    if fallback:
                        fallback_ai_text = fallback
    except Exception as exc:
        if assistant_full_text:
            yield {"event": "error", "detail": f"stream interrupted: {exc}"}
        else:
            yield {"event": "error", "detail": f"stream failed: {exc}"}

    if stream_buffer:
        tail = _strip_markdown_syntax(stream_buffer)
        if not _is_internal_leak_line(tail):
            assistant_full_text += tail
            yield {"event": "delta", "text": tail}

    if not assistant_full_text and fallback_ai_text:
        assistant_full_text = _sanitize_user_facing_text(fallback_ai_text)
        if assistant_full_text:
            yield {"event": "delta", "text": assistant_full_text}

    assistant_full_text = _sanitize_user_facing_text(assistant_full_text)

    updated_history = _to_deepagent_messages(history, message)
    updated_history.append({"role": "assistant", "content": assistant_full_text})
    _dispatch_memory_agent(user_message=message, assistant_message=assistant_full_text)
    yield {
        "event": "done",
        "reply": assistant_full_text,
        "thread_id": resolved_thread_id,
        "history": updated_history,
    }


class ChatRequest(BaseModel):
    message: str
    history: list[Any] = Field(default_factory=list)
    thread_id: str | None = None


def _to_sse(event: dict[str, Any]) -> str:
    payload = json.dumps(event, ensure_ascii=False)
    return f"data: {payload}\n\n"


def create_app() -> FastAPI:
    app = FastAPI(title="AI Chat Platform", version="1.0.0")
    app.mount("/static", StaticFiles(directory=str(FRONTEND_DIR)), name="static")

    # åº”ç”¨å¯åŠ¨æ—¶åˆå§‹åŒ– Agentsï¼ˆå¸¦å¤šè¿›ç¨‹ä¿æŠ¤ï¼‰
    init_agents()
    app = FastAPI(title="AI Chat Platform", version="1.0.0")
    app.mount("/static", StaticFiles(directory=str(FRONTEND_DIR)), name="static")

    @app.get("/api/health")
    def health() -> dict[str, str]:
        return {"status": "ok", "model": model_name}

    @app.get("/api/skills")
    def skills() -> dict[str, Any]:
        return {"skills": _list_skill_packages(PROJECT_ROOT / "skills")}

    @app.get("/api/customers")
    def customers() -> dict[str, Any]:
        return {
            "customers": _customers_payload(PROJECT_ROOT / "data" / "customer_db.csv")
        }

    @app.get("/api/memories")
    def memories() -> dict[str, Any]:
        return _memories_payload()

    @app.get("/api/countdown/{task_id}")
    async def get_countdown_status_api(task_id: str) -> dict[str, Any]:
        """ä»£ç†æŸ¥è¯¢ MCP æœåŠ¡çš„å€’è®¡æ—¶çŠ¶æ€"""
        import urllib.error
        import urllib.request

        try:
            # è°ƒç”¨ MCP æœåŠ¡çš„ get_countdown_status å·¥å…·
            # MCP é€šè¿‡ /messages/ ç«¯ç‚¹æ¥æ”¶ JSON-RPC è¯·æ±‚
            request_body = json.dumps(
                {
                    "jsonrpc": "2.0",
                    "id": 1,
                    "method": "tools/call",
                    "params": {
                        "name": "get_countdown_status",
                        "arguments": {"task_id": task_id},
                    },
                }
            ).encode("utf-8")

            req = urllib.request.Request(
                f"{COUNTDOWN_MCP_URL}/messages/",
                data=request_body,
                headers={"Content-Type": "application/json"},
                method="POST",
            )

            with urllib.request.urlopen(req, timeout=5) as response:
                response_data = json.loads(response.read().decode("utf-8"))
                # è§£æ MCP å“åº”
                if "result" in response_data:
                    result = response_data["result"]
                    if isinstance(result, dict) and "content" in result:
                        content = result["content"]
                        if isinstance(content, list) and len(content) > 0:
                            text_item = content[0]
                            if isinstance(text_item, dict) and "text" in text_item:
                                data = json.loads(text_item["text"])
                                return data

                # å¦‚æœæ— æ³•è§£æï¼Œè¿”å›é”™è¯¯
                return {"status": "error", "message": "æ— æ³•è§£æ MCP å“åº”"}

        except urllib.error.HTTPError as e:
            if e.code == 404:
                return {"status": "not_found", "task_id": task_id}
            return {"status": "error", "message": f"MCP æœåŠ¡é”™è¯¯: {e.code}"}
        except Exception as e:
            return {"status": "error", "message": f"æŸ¥è¯¢å¤±è´¥: {str(e)}"}

    @app.get("/")
    def index() -> HTMLResponse:
        html = (FRONTEND_DIR / "index.html").read_text(encoding="utf-8")
        version = _frontend_asset_version()
        rendered = html.replace("__ASSET_VERSION__", version)
        return HTMLResponse(rendered)

    @app.post("/api/chat")
    def chat(payload: ChatRequest) -> dict[str, Any]:
        if not payload.message.strip():
            raise HTTPException(status_code=400, detail="message cannot be empty")

        try:
            answer, thread_id, history = chat_with_agent(
                message=payload.message,
                history=payload.history,
                thread_id=payload.thread_id,
            )
        except Exception as exc:
            raise HTTPException(status_code=500, detail=f"chat failed: {exc}") from exc

        return {
            "reply": answer,
            "thread_id": thread_id,
            "history": history,
        }

    @app.post("/api/chat/stream")
    async def chat_stream(payload: ChatRequest) -> StreamingResponse:
        if not payload.message.strip():
            raise HTTPException(status_code=400, detail="message cannot be empty")

        async def event_generator() -> AsyncIterator[str]:
            async for event in stream_chat_with_agent(
                message=payload.message,
                history=payload.history,
                thread_id=payload.thread_id,
            ):
                yield _to_sse(event)

        return StreamingResponse(event_generator(), media_type="text/event-stream")

    return app


app = create_app()


def main() -> None:
    auto_reload = os.getenv("AUTO_RELOAD", "1") == "1"
    port = int(os.getenv("PORT", "7860"))

    # Agents å·²åœ¨ create_app() ä¸­åˆå§‹åŒ–
    # ä¸éœ€è¦åœ¨è¿™é‡Œå†æ¬¡è°ƒç”¨ init_agents()

    # æ³¨å†Œä¿¡å·å¤„ç†ç¨‹åºï¼Œç¡®ä¿ç¨‹åºé€€å‡ºæ—¶æ¸…ç†æ²™ç®±
    auto_reload = os.getenv("AUTO_RELOAD", "1") == "1"
    port = int(os.getenv("PORT", "8005"))

    # åœ¨ä¸»è¿›ç¨‹ä¸­é¢„åŠ è½½ Agentsï¼ˆåœ¨ uvicorn å¯åŠ¨å‰ï¼‰
    print("ğŸš€ ä¸»è¿›ç¨‹ï¼šé¢„åŠ è½½ Agents...")
    init_agents()
    print("âœ… Agents é¢„åŠ è½½å®Œæˆï¼Œå¯åŠ¨ Uvicorn...\n")

    # æ³¨å†Œä¿¡å·å¤„ç†ç¨‹åºï¼Œç¡®ä¿ç¨‹åºé€€å‡ºæ—¶æ¸…ç†æ²™ç®±
    import signal

    def signal_handler(signum, frame):
        print(f"\næ¥æ”¶åˆ°ä¿¡å· {signum}ï¼Œæ­£åœ¨å…³é—­...")
        cleanup_daytona()
        sys.exit(0)

    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

    try:
        uvicorn.run("main:app", host="0.0.0.0", port=port, reload=auto_reload)
    finally:
        # ç¡®ä¿æ²™ç®±è¢«æ¸…ç†
        cleanup_daytona()


if __name__ == "__main__":
    main()
